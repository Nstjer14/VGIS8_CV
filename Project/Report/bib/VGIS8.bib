Automatically generated by Mendeley Desktop 1.17.13
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Percy,
author = {Percy, Oad},
file = {::},
journal = {Blekinge Institute of Technology},
pages = {1--48},
title = {{Iris localization using Daugman ' s algorithm}},
url = {http://www.diva-portal.org/smash/get/diva2:831173/FULLTEXT01.pdf}
}
@article{Fierrez-Aguilar2003a,
abstract = {The aim of this paper, regarding multimodal biometric verification, is twofold: on the one hand, some score fusion strategies reported in the literature are reviewed and, on the other hand, we compare experimentally a selection of them using as monomodal baseline experts: i) our face verification system based on a global face appearance representation scheme, ii) our minutiaebased fingerprint verification system, and iii) our on-line signature verification system based on HMM modeling of temporal functions, on the MCYT multimodal database. A new strategy is also proposed and discussed in order to generate a multimodal combined score by means of Support Vector Machine (SVM) classifiers from which user-independent and user-dependent fusion schemes are derived and evaluated.},
author = {Fierrez-Aguilar, J and Ortega-Garcia, J and Garcia-Romero, D and Gonzalez-Rodriguez, J},
isbn = {3-540-40302-7},
issn = {03029743},
journal = {Proc. AVBPA},
pages = {1056},
title = {{A Comparative Evaluation of Fusion Strategies for Multimodal Biometric Verification}},
year = {2003}
}
@article{Cheng2017a,
abstract = {This paper focuses on indoor semantic segmentation us-ing RGB-D data. Although the commonly used deconvolu-tion networks (DeconvNet) have achieved impressive results on this task, we find there is still room for improvements in two aspects. One is about the boundary segmentation. DeconvNet aggregates large context to predict the label of each pixel, inherently limiting the segmentation precision of object boundaries. The other is about RGB-D fusion. Re-cent state-of-the-art methods generally fuse RGB and depth networks with equal-weight score fusion, regardless of the varying contributions of the two modalities on delineating different categories in different scenes. To address the two problems, we first propose a locality-sensitive DeconvNet (LS-DeconvNet) to refine the boundary segmentation over each modality. LS-DeconvNet incorporates locally visual and geometric cues from the raw RGB-D data into each DeconvNet, which is able to learn to upsample the coarse convolutional maps with large context whilst recovering sharp object boundaries. Towards RGB-D fusion, we introduce a gated fusion layer to effectively combine the two LS-DeconvNets. This layer can learn to adjust the contributions of RGB and depth over each pixel for high-performance object recognition. Experiments on the large-scale SUN RGB-D dataset and the popular NYU-Depth v2 dataset show that our approach achieves new state-of-the-art results for RGB-D indoor semantic segmentation.},
author = {Cheng, Yanhua and Cai, Rui and Li, Zhiwei and Zhao, Xin and Huang, Kaiqi},
doi = {10.1109/CVPR.2017.161},
isbn = {978-1-5386-0457-1},
journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {1475--1483},
title = {{Locality-Sensitive Deconvolution Networks with Gated Fusion for RGB-D Indoor Semantic Segmentation}},
url = {http://ieeexplore.ieee.org/document/8099644/},
year = {2017}
}
@article{Daia,
abstract = {Deep networks have shown impressive performance on many computer vision tasks. Recently, deep convolutional neural networks (CNNs) have been used to learn discrim-inative texture representations. One of the most successful approaches is Bilinear CNN model that explicitly captures the second order statistics within deep features. However, these networks cut off the first order information flow in the deep network and make gradient back-propagation dif-ficult. We propose an effective fusion architecture -FASON that combines second order information flow and first or-der information flow. Our method allows gradients to back-propagate through both flows freely and can be trained ef-fectively. We then build a multi-level deep architecture to exploit the first and second order information within dif-ferent convolutional layers. Experiments show that our method achieves improvements over state-of-the-art meth-ods on several benchmark datasets.},
author = {Dai, Xiyang and Ng, Joe Yue-hei and Davis, Larry S},
journal = {Cvpr2017},
pages = {7352--7360},
title = {{FASON : First and Second Order Information Fusion Network for Texture Recognition}}
}
@inproceedings{Zhao2015a,
abstract = {This paper proposes a novel and more accurate iris segmentation framework to automatically segment iris region from the face images acquired with relaxed imaging under visible or near-infrared illumination, which provides strong feasibility for applications in surveillance, forensics and the search for missing children, etc. The proposed framework is built on a novel total-variation based formulation which uses l1 norm regularization to robustly suppress noisy texture pixels for the accurate iris localization. A series of novel and robust post processing operations are introduced to more accurately localize the limbic boundaries. Our experimental results on three publicly available databases, i.e., FRGC, UBIRIS.v2 and CASIA.v4-distance, achieve significant performance improvement in terms of iris segmentation accuracy over the state-of-the-art approaches in the literature. Besides, we have shown that using iris masks generated from the proposed approach helps to improve iris recognition performance as well. Unlike prior work, all the implementations in this paper are made publicly available to further advance research and applications in biometrics at-d-distance.},
author = {Zhao, Zijing and Kumar, Ajay},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2015.436},
file = {::},
isbn = {9781467383912},
issn = {15505499},
month = {dec},
pages = {3828--3836},
publisher = {IEEE},
title = {{An accurate iris segmentation framework under relaxed imaging constraints using total variation model}},
url = {http://ieeexplore.ieee.org/document/7410793/},
volume = {2015 Inter},
year = {2015}
}
@article{Baltrusaitis2017a,
abstract = {Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.},
archivePrefix = {arXiv},
arxivId = {1705.09406},
author = {Baltru{\v{s}}aitis, Tadas and Ahuja, Chaitanya and Morency, Louis-Philippe},
doi = {10.1109/TPAMI.2018.2798607},
eprint = {1705.09406},
issn = {0162-8828},
number = {c},
pages = {1--20},
title = {{Multimodal Machine Learning: A Survey and Taxonomy}},
url = {http://arxiv.org/abs/1705.09406},
volume = {8828},
year = {2017}
}
@article{Fierrez2018c,
abstract = {The present paper is Part 2 in this series of two papers. In Part 1 we provided an introduction to Multiple Classifier Systems (MCS) with a focus into the fundamentals: basic nomenclature, key elements, architecture, main methods, and prevalent theory and framework. Part 1 then overviewed the application of MCS to the particular field of multimodal biometric person authentication in the last 25 years, as a prototypical area in which MCS has resulted in important achievements. Here in Part 2 we present in more technical detail recent trends and developments in MCS coming from multimodal biometrics that incorporate context information in an adaptive way. These new MCS architectures exploit input quality measures and pattern-specific particularities that move apart from general population statistics, resulting in robust multimodal biometric systems. Similarly as in Part 1, methods here are described in a general way so they can be applied to other information fusion problems as well. Finally, we also discuss here open challenges in biometrics in which MCS can play a key role.},
author = {Fierrez, Julian and Morales, Aythami and Vera-Rodriguez, Ruben and Camacho, David},
doi = {10.1016/j.inffus.2017.12.005},
issn = {15662535},
journal = {Information Fusion},
keywords = {Adaptive,Biometrics,Classifier,Context,Fusion,Multimodal},
number = {November 2017},
pages = {103--112},
publisher = {Elsevier},
title = {{Multiple classifiers in biometrics. Part 2: Trends and challenges}},
url = {https://doi.org/10.1016/j.inffus.2017.12.005},
volume = {44},
year = {2018}
}
@misc{LiborMasek2003,
annote = {Open Source Matlab Iris Recognition system. Based on Daugmans approach.},
author = {{Libor Masek}, Peter Kovesi},
publisher = {The School of Computer Science and Software Engineering, The University of Western Australia},
title = {{MATLAB Source Code for a Biometric Identification System Based on Iris Patterns}},
url = {http://www.peterkovesi.com/studentprojects/libor/sourcecode.html},
year = {2003}
}
@article{Li2017,
abstract = {This paper investigates how to integrate the complementary information from RGB and thermal (RGB-T) sources for object tracking. We propose a novel Convolutional Neural Network (ConvNet) architecture, including a two-stream ConvNet and a FusionNet, to achieve adaptive fusion of different source data for robust RGB-T tracking. Both RGB and thermal streams extract generic semantic information of the target object. In particular, the thermal stream is pre-trained on the ImageNet dataset to encode rich semantic information, and then fine-tuned using thermal images to capture the specific properties of thermal information. For adaptive fusion of different modalities while avoiding redundant noises, the FusionNet is employed to select most discriminative feature maps from the outputs of the two-stream ConvNet, and updated online to adapt to appearance variations of the target object. Finally, the object locations are efficiently predicted by applying the multi-channel correlation filter on the fused feature maps. Extensive experiments on the recently public benchmark GTOT verify the effectiveness of the proposed approach against other state-of-the-art RGB-T trackers.},
author = {Li, Chenglong and Wu, Xiaohao and Zhao, Nan and Cao, Xiaochun and Tang, Jin},
doi = {10.1016/j.neucom.2017.11.068},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Adaptive fusion,Convolutional neural network,Correlation filter,Object tracking,Thermal information},
pages = {78--85},
publisher = {Elsevier B.V.},
title = {{Fusing two-stream convolutional neural networks for RGB-T object tracking}},
url = {https://doi.org/10.1016/j.neucom.2017.11.068},
volume = {281},
year = {2017}
}
@article{Hqwhu,
author = {Hqwhu, E and Vndudkdq, Qvndudnrf and Jwx, Dnjxo and Wu, H G X},
file = {::},
keywords = {convolutional neural,deep learning,network,pupil center estimation},
title = {{Deep learning based estimation of the eye pupil center by using image patch classification}}
}
@article{Crossdata2018,
abstract = {In this paper we study face recognition using convolutional neural network. First, we introduced the basic CNN neural network architecture. Second, we modify the traditional neural network and adapt it to another database by fine tuning its parameters. Third, the network architecture is extended to the cross database problem. The CNN is first trained on a large dataset and then tested on another. Experimental results show that the proposed algorithm is suitable for building various real world applications.},
author = {Guo, Mei and Xiao, Min and Gong, Deliang},
doi = {10.1007/978-3-319-69096-4_54},
file = {:C$\backslash$:/Users/Shaggy/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Guo, Xiao, Gong - Unknown - Face Recognition Using Deep Convolutional Neural Network in Cross-Database Study.pdf:pdf},
journal = {Advances in Intelligent Systems and Computing},
keywords = {Deep neural network {\'{A}},Face recognition {\'{A}},Image processing},
title = {{Face Recognition Using Deep Convolutional Neural Network in Cross-Database Study}},
url = {https://link-springer-com.zorac.aub.aau.dk/content/pdf/10.1007{\%}2F978-3-319-69096-4{\_}54.pdf},
year = {2017}
}
@article{Pathak2016a,
author = {Pathak, Mrunal and Srinivasu, N},
doi = {10.1007/978-981-10-2630-0},
isbn = {978-981-10-2629-4},
keywords = {biometrics},
pages = {137--152},
title = {{Advances in Computing Applications}},
url = {http://link.springer.com/10.1007/978-981-10-2630-0},
year = {2016}
}
@article{Wang2009a,
abstract = {Feature-level fusion remains a challenging problem for multimodal biometrics. However, existing fusion schemes such as sum rule and weighted sum rule are inefficient in complicated condition. In this paper, we propose an efficient feature-level fusion algorithm for iris and face in parallel. The algorithm first normalizes the original features of iris and face using z-score model, and then take complex FDA as the classifier of Unitary space. The proposed algorithm is tested using CASIA iris database and two face databases (ORL database and Yale database.). Experimental results show the effectiveness of the proposed algorithm.},
author = {Wang, Zhifang and Han, Qi and Niu, Xiamu and Busch, Christoph},
doi = {10.1007/978-3-642-01513-7_38},
isbn = {3642015123},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Biometrics,CFDA,Feature-level,Parallel fusion,Unitary space},
number = {PART 3},
pages = {356--364},
title = {{Feature-level fusion of Iris and face for personal identification}},
volume = {5553 LNCS},
year = {2009}
}
@article{Khiari-Hili2017a,
author = {Khiari-Hili, Nefissa and Montagne, Christophe and Lelandais, Sylvie and Hamrouni, Kamel},
doi = {10.1109/IPTA.2016.7820954},
isbn = {9781467389105},
journal = {2016 6th International Conference on Image Processing Theory, Tools and Applications, IPTA 2016},
keywords = {Multimodal biometrics,authentication,dynamic weighted sum,face,iris,quality,score fusion},
pages = {1--6},
title = {{Quality dependent multimodal fusion of face and iris biometrics}},
year = {2017}
}
@article{Phillips2009,
abstract = {The goal of the Multiple Biometrics Grand Challenge (MBGC) is to improve the performance of face and iris recognition technology from biometric samples acquired under unconstrained conditions. The MBGC is organized into three challenge problems. Each challenge problem re- laxes the acquisition constraints in different directions. In the Portal Challenge Problem, the goal is to recognize people from near-infrared (NIR) and high definition (HD) video as they walk through a portal. Iris recognition can be performed from the NIR video and face recognition from the HD video. The availability of NIR and HD modalities allows for the development of fusion algorithms. The Still Face Challenge Problem has two primary goals. The first is to improve recognition performance from frontal and off angle still face images taken under uncontrolled in- door and outdoor lighting. The second is to improve recognition perfor- mance on still frontal face images that have been resized and compressed, as is required for electronic passports. In the Video Challenge Problem, the goal is to recognize people from video in unconstrained environments. The video is unconstrained in pose, illumination, and camera angle. All three challenge problems include a large data set, experiment descrip- tions, ground truth, and scoring code.},
author = {Phillips, P. Jonathon and Flynn, Patrick J. and Beveridge, J. Ross and Scruggs, W. Todd and O'Toole, Alice J. and Bolme, David and Bowyer, Kevin W. and Draper, Bruce A. and Givens, Geof H. and Lui, Yui Man and Sahibzada, Hassan and Scallan, Joseph A. and Weimer, Samuel},
doi = {10.1007/978-3-642-01793-3_72},
isbn = {3642017924},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {705--714},
title = {{Overview of the multiple biometrics grand challenge}},
volume = {5558 LNCS},
year = {2009}
}
@inproceedings{Eitel2015,
abstract = {Robust object recognition is a crucial ingredient of many, if not all, real-world robotics applications. This paper leverages recent progress on Convolutional Neural Networks (CNNs) and proposes a novel RGB-D architecture for object recognition. Our architecture is composed of two separate CNN processing streams - one for each modality - which are consecutively combined with a late fusion network. We focus on learning with imperfect sensor data, a typical problem in real-world robotics tasks. For accurate learning, we introduce a multi-stage training methodology and two crucial ingredients for handling depth data with CNNs. The first, an effective encoding of depth information for CNNs that enables learning without the need for large depth datasets. The second, a data augmentation scheme for robust learning with depth images by corrupting them with realistic noise patterns. We present state-of-the-art results on the RGB-D object dataset and show recognition in challenging RGB-D real-world noisy settings.},
archivePrefix = {arXiv},
arxivId = {1507.06821},
author = {Eitel, Andreas and Springenberg, Jost Tobias and Spinello, Luciano and Riedmiller, Martin and Burgard, Wolfram},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2015.7353446},
eprint = {1507.06821},
file = {::},
isbn = {9781479999941},
issn = {21530866},
month = {jul},
pages = {681--687},
title = {{Multimodal deep learning for robust RGB-D object recognition}},
url = {http://arxiv.org/abs/1507.06821},
volume = {2015-Decem},
year = {2015}
}
@article{Nam2016a,
abstract = {We propose Dual Attention Networks (DANs) which jointly leverage visual and textual attention mechanisms to capture fine-grained interplay between vision and language. DANs attend to specific regions in images and words in text through multiple steps and gather essential information from both modalities. Based on this framework, we introduce two types of DANs for multimodal reasoning and matching, respectively. The reasoning model allows visual and textual attentions to steer each other during collaborative inference, which is useful for tasks such as Visual Question Answering (VQA). In addition, the matching model exploits the two attention mechanisms to estimate the similarity between images and sentences by focusing on their shared semantics. Our extensive experiments validate the effectiveness of DANs in combining vision and language, achieving the state-of-the-art performance on public benchmarks for VQA and image-text matching.},
archivePrefix = {arXiv},
arxivId = {1611.00471},
author = {Nam, Hyeonseob and Ha, Jung-Woo and Kim, Jeonghee},
doi = {10.1109/CVPR.2017.232},
eprint = {1611.00471},
isbn = {978-1-5386-0457-1},
issn = {1611.00471},
pages = {299--307},
title = {{Dual Attention Networks for Multimodal Reasoning and Matching}},
url = {http://arxiv.org/abs/1611.00471},
year = {2016}
}
@article{Nielsen2015,
abstract = {Neural Networks and Deep Learning is a free online book. The book will teach you about: Neural networks, a beautiful biologically-inspired programming paradigm which enables a computer to learn from observational data Deep learning, a powerful set of techniques for learning in neural networks Neural networks and deep learning currently provide the best solutions to many problems in image recognition, speech recognition, and natural language processing. This book will teach you many of the core concepts behind neural networks and deep learning.},
author = {Nielsen, Michael},
journal = {Determination Press},
title = {{Neural Networks and Deep Learning}},
url = {http://neuralnetworksanddeeplearning.com/index.html},
year = {2015}
}
@article{Liu2013a,
author = {Liu, Jing and Sun, Zhenan and Tan, Tieniu},
doi = {10.1109/BTAS.2013.6712692},
isbn = {9781479905270},
journal = {IEEE 6th International Conference on Biometrics: Theory, Applications and Systems, BTAS 2013},
pages = {1--6},
title = {{Code-level information fusion of low-resolution iris image sequences for personal identification at a distance}},
year = {2013}
}
@article{Rifaee2017,
author = {Rifaee, Mustafa and Abdallah, Mohammad and Okosh, Basem},
file = {:C$\backslash$:/Users/Shaggy/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rifaee, Mustafa and Abdallah, Mohammad and Okosh - 2017 - A Short Survey for Iris Images Databases.pdf:pdf},
journal = {international journal of multimedia {\&} its applications},
number = {April},
title = {{A Short Survey for Iris Images Databases}},
url = {https://www.researchgate.net/publication/316093004{\_}A{\_}Short{\_}Survey{\_}for{\_}Iris{\_}Images{\_}Databases},
year = {2017}
}
@inproceedings{Ghazi2016,
abstract = {Deep learning based approaches have been dominating the face recognition field due to the significant performance improvement they have provided on the challenging wild datasets. These approaches have been extensively tested on such unconstrained datasets, on the Labeled Faces in the Wild and YouTube Faces, to name a few. However, their capability to handle individual appearance variations caused by factors such as head pose, illumination, occlusion, and misalignment has not been thoroughly assessed till now. In this paper, we present a comprehensive study to evaluate the performance of deep learning based face representation under several conditions including the varying head pose angles, upper and lower face occlusion, changing illumination of different strengths, and misalignment due to erroneous facial feature localization. Two successful and publicly available deep learning models, namely VGG-Face and Lightened CNN have been utilized to extract face representations. The obtained results show that although deep learning provides a powerful representation for face recognition, it can still benefit from preprocessing, for example, for pose and illumination normalization in order to achieve better performance under various conditions. Particularly, if these variations are not included in the dataset used to train the deep learning model, the role of preprocessing becomes more crucial. Experimental results also show that deep learning based representation is robust to misalignment and can tolerate facial feature localization errors up to 10{\%} of the interocular distance.},
author = {Ghazi, Mostafa Mehdipour and Ekenel, Hazim Kemal},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
doi = {10.1109/CVPRW.2016.20},
file = {::},
isbn = {978-1-5090-1437-8},
month = {jun},
pages = {102--109},
publisher = {IEEE},
title = {{A Comprehensive Analysis of Deep Learning Based Representation for Face Recognition}},
url = {http://ieeexplore.ieee.org/document/7789510/},
year = {2016}
}
@article{Karpathy2016,
abstract = {These notes accompany the Stanford CS class CS231n: Convolutional Neural Networks for Visual Recognition.},
author = {Karpathy, Andrej},
journal = {Stanford University},
pages = {1--21},
title = {{Convolutional Neural Networks for Visual Recognition}},
url = {http://cs231n.github.io/classification/},
year = {2016}
}
@article{Krizhevsky2017a,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%}, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, con-sists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed reg-ularization method called " dropout " that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the sec-ond-best entry. 1. PROLOGUE Four years ago, a paper by Yann LeCun and his collaborators was rejected by the leading computer vision conference on the grounds that it used neural networks and therefore pro-vided no insight into how to design a vision system. At the time, most computer vision researchers believed that a vision system needed to be carefully hand-designed using a detailed understanding of the nature of the task. They assumed that the task of classifying objects in natural images would never be solved by simply presenting examples of images and the names of the objects they contained to a neural network that acquired all of its knowledge from this training data. What many in the vision research community failed to appreciate was that methods that require careful hand-engi-neering by a programmer who understands the domain do not scale as well as methods that replace the programmer with a powerful general-purpose learning procedure. With enough computation and enough data, learning beats pro-gramming for complicated tasks that require the integration of many different, noisy cues. Four years ago, while we were at the University of Toronto, our deep neural network called SuperVision almost halved the error rate for recognizing objects in natural images and triggered an overdue paradigm shift in computer vision. Figure 4 shows some examples of what SuperVision can do. SuperVision evolved from the multilayer neural networks that were widely investigated in the 1980s. These networks used multiple layers of feature detectors that were all learned from the training data. Neuroscientists and psychologists had hypothesized that a hierarchy of such feature detectors would provide a robust way to recognize objects but they had no idea how such a hierarchy could be learned. There was great excite-ment in the 1980s because several different research groups discovered that multiple layers of feature detectors could be trained efficiently using a relatively straight-forward algorithm called backpropagation},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
doi = {10.1145/3065386},
file = {::},
journal = {COMMUNICATIONS OF THE ACM},
number = {6},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
url = {http://delivery.acm.org/10.1145/3070000/3065386/p84-krizhevsky.pdf?ip=130.225.198.196{\&}id=3065386{\&}acc=OA{\&}key=36332CD97FA87885.1DDFD8390336D738.4D4702B0C3E38B35.5945DC2EABF3343C{\&}{\_}{\_}acm{\_}{\_}=1518530104{\_}1e0b97ec1deaadc1937d34c07a1392ac},
volume = {60},
year = {2017}
}
@article{Johnson2010a,
abstract = {Identification of individuals using biometric information has found great success in many security and law enforcement applications. Up until the present time, most research in the field has been focused on ideal conditions and most available databases are constructed in these ideal conditions. There has been a growing interest in the perfection of these technologies at a distance and in less than ideal conditions, i.e. low lighting, out-of-focus blur, off angles, etc. This paper presents a dataset consisting of face and iris videos obtained at distances of 5 to 25 feet and in conditions of varying quality. The purpose of this database is to set a standard for quality measurement in face and iris data and to provide a means for analyzing biométrie systems in less than ideal conditions. The structure of the dataset as well as a quantified metric for quality measurement based on a 25 subject subset of the dataset is presented.},
author = {Johnson, P. A. and Lopez-Meyer, P. and Sazonova, N. and Hua, F. and Schuckers, S.},
doi = {10.1109/BTAS.2010.5634513},
isbn = {9781424475803},
journal = {IEEE 4th International Conference on Biometrics: Theory, Applications and Systems, BTAS 2010},
title = {{Quality in face and Iris research ensemble (Q-FIRE)}},
year = {2010}
}
@article{Mhaske2013a,
author = {Mhaske, V. D. and Patankar, A. J.},
doi = {10.1109/ICCIC.2013.6724125},
isbn = {9781479915972},
journal = {2013 IEEE International Conference on Computational Intelligence and Computing Research, IEEE ICCIC 2013},
keywords = {Biometrics,Fingerprint,Fusion,MGF,Multimodal,Palm print,ROI,Unimodal},
title = {{Multimodal biometrics by integrating fingerprint and palmprint for security}},
year = {2013}
}
@article{Jung2017,
abstract = {Finding the accurate position of an eye is crucial for mobile iris recognition system in order to extract the iris region quickly and correctly. Unfortunately, this is very difficult to accomplish when a person is wearing eyeglasses because of the interference from the eyeglasses. This paper proposes an eye detection method that is robust to eyeglass interference in mobile environment. The proposed method comprises two stages: eye candidate generation and eye validation. In the eye candidate generation stage, multi-scale window masks consisting of 2 × 3 subblocks are used to generate all image blocks possibly containing an eye image. In the ensuing eye validation stage, two methods are employed to determine which blocks actually contain true eye images and locate their precise positions as well: the first method searches for the glint of an NIR illuminator on the pupil region. If this first method fails, the next method computes the intensity difference between the assumed pupil and its surrounding region using multi-scale 3 × 3 window masks. Experimental results show that the proposed method detects the eye position more accurately and quickly than competing methods in the presence of interference from eyeglass frames.},
author = {Jung, Yujin and Kim, Dongik and Son, Byungjun and Kim, Jaihie},
doi = {10.1016/j.eswa.2016.09.036},
file = {:C$\backslash$:/Users/Shaggy/Desktop/Iris Articles/An eye detection method robust to eyeglasses for mobile iris recognition.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Eye detection,Eye validation,Eyeglasses,Iris detection,Iris recognition,Mobile},
pages = {178--188},
publisher = {Elsevier Ltd},
title = {{An eye detection method robust to eyeglasses for mobile iris recognition}},
url = {http://dx.doi.org/10.1016/j.eswa.2016.09.036},
volume = {67},
year = {2017}
}
@inproceedings{Sangeetha2013,
abstract = {-In a Multimodal biometric system, the effective fusion method is necessary for combining information from various single modality systems. Two biometric characteristics are considered in this study: iris and fingerprint. Multimodal biometric system needs an effective fusion scheme to combine biometric characteristics derived from one or more modalities. The score level fusion is used to combine the characteristics from diff erent biometric modalities. Fusion at the score level is a new technique, which has a high potential for e f ficient consolidation of multiple unimodal biometric matcher outputs. Support vector machine and extreme learning techniques are used in this system for recognition of biometric traits. In this, the Fingerprint-Iris system provides better performance and comparison of support vector machine and extreme learning machine based on score-level fusion methods is obtained In score-level fusion, ELM provides better performance as compare to the SVM It reduces the classification time of current system. This work is valuable and makes an e f ficient accuracy in such applications. This system can be utilized for person identification in several applications.},
author = {Sangeetha, S and Radha, N.},
booktitle = {2013 7th International Conference on Intelligent Systems and Control (ISCO)},
doi = {10.1109/ISCO.2013.6481145},
isbn = {978-1-4673-4603-0},
month = {jan},
pages = {183--188},
publisher = {IEEE},
title = {{A New Framework for IRIS and Fingerprint Recognition Using SVM Classification and Extreme Learning Machine Based on Score Level Fusion}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6481145},
year = {2013}
}
@article{Kuehlkamp2016,
abstract = {Iris recognition systems are a mature technology that is widely used throughout the world. In identification (as opposed to verification) mode, an iris to be recognized is typically matched against all N enrolled irises. This is the classic "1-to-N search". In order to improve the speed of large-scale identification, a modified "1-to-First" search has been used in some operational systems. A 1-to-First search terminates with the first below-threshold match that is found, whereas a 1-to-N search always finds the best match across all enrollments. We know of no previous studies that evaluate how the accuracy of 1-to-First search differs from that of 1-to-N search. Using a dataset of over 50,000 iris images from 2,800 different irises, we perform experiments to evaluate the relative accuracy of 1-to-First and 1-to-N search. We evaluate how the accuracy difference changes with larger numbers of enrolled irises, and with larger ranges of rotational difference allowed between iris images. We find that False Match error rate for 1-to-First is higher than for 1-to-N, and the the difference grows with larger number of enrolled irises and with larger range of rotation.},
annote = {The way that iris recognition works is that some kind of filter is applied to localize the iris. 2D Gador filter is often cited. Then that iris is checked against the whole database. This is called 1:N. They check the Hamming Distance between of the bits of the data and then choose the lowest ones as a pair. In 1:First search they do the same except there is is a threshold that that is has to be under to be accepted. If it is under that threshold it is accepted and the search is stopped. Two types of error can occour: a False Match (FM) and False Non-Match (FNM). A false match occurs when two samples from different individuals are declared by the system as a match. A false non-match is when two samples from the same individual fail to be considered as a match by the system},
archivePrefix = {arXiv},
arxivId = {1702.01167},
author = {Kuehlkamp, Andrey and Bowyer, Kevin W.},
doi = {10.1109/WACV.2016.7477687},
eprint = {1702.01167},
file = {:C$\backslash$:/Users/Shaggy/Desktop/Iris Articles/An analysis of 1-to-first matching in iris recognition.pdf:pdf},
isbn = {9781509006410},
journal = {2016 IEEE Winter Conference on Applications of Computer Vision, WACV 2016},
title = {{An analysis of 1-to-first matching in iris recognition}},
year = {2016}
}
@article{sun2015,
abstract = {The state-of-the-art of face recognition has been significantly advanced by the emergence of deep learning. Very deep neural networks recently achieved great success on general object recognition because of their superb learning capacity. This motivates us to investigate their effectiveness on face recognition. This paper proposes two very deep neural network architectures, referred to as DeepID3, for face recognition. These two architectures are rebuilt from stacked convolution and inception layers proposed in VGG net and GoogLeNet to make them suitable to face recognition. Joint face identification-verification supervisory signals are added to both intermediate and final feature extraction layers during training. An ensemble of the proposed two architectures achieves 99.53{\%} LFW face verification accuracy and 96.0{\%} LFW rank-1 face identification accuracy, respectively. A further discussion of LFW face verification result is given in the end.},
archivePrefix = {arXiv},
arxivId = {1502.00873},
author = {Sun, Yi and Liang, Ding and Wang, Xiaogang and Tang, Xiaoou},
eprint = {1502.00873},
file = {::},
title = {{DeepID3: Face Recognition with Very Deep Neural Networks}},
url = {https://arxiv.org/pdf/1502.00873.pdf http://arxiv.org/abs/1502.00873},
year = {2015}
}
@inproceedings{deepID2014,
abstract = {The key challenge of face recognition is to develop effective feature representations for reducing intra-personal variations while enlarging inter-personal differences. In this paper, we show that it can be well solved with deep learning and using both face identification and verification signals as supervision. The Deep IDentification-verification features (DeepID2) are learned with carefully designed deep convolutional networks. The face identification task increases the inter-personal variations by drawing DeepID2 extracted from different identities apart, while the face verification task reduces the intra-personal variations by pulling DeepID2 extracted from the same identity together, both of which are essential to face recognition. The learned DeepID2 features can be well generalized to new identities unseen in the training data. On the challenging LFW dataset, 99.15{\%} face verification accuracy is achieved. Compared with the best deep learning result on LFW, the error rate has been significantly reduced by 67{\%}.},
archivePrefix = {arXiv},
arxivId = {1406.4773},
author = {Sun, Yi and Wang, Xiaogang and Tang, Xiaoou},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2014.244},
eprint = {1406.4773},
file = {::},
isbn = {9781479951178},
issn = {10636919},
keywords = {deep learning,face verification},
pages = {1891--1898},
pmid = {21808091},
title = {{Deep learning face representation from predicting 10,000 classes}},
url = {http://mmlab.ie.cuhk.edu.hk/pdf/YiSun{\_}CVPR14.pdf},
year = {2014}
}
@article{BiosecID2008,
abstract = {A new multimodal biometric database, acquired in the framework of the BiosecurID project, is presented together with the description of the acquisition setup and protocol. The database includes eight unimodal biometric traits, namely: speech, iris, face (still images, videos of talking faces), handwritten signature and handwritten text (on-line dynamic signals, off-line scanned images), fingerprints (acquired with two different sensors), hand (palmprint, contour-geometry) and keystroking. The database comprises 400 subjects and presents features such as: realistic acquisition scenario, balanced gender and population distributions, availability of information about particular demographic groups (age, gender, handedness), acquisition of replay attacks for speech and keystroking, skilled forgeries for signatures, and compatibility with other existing databases. All these characteristics make it very useful in research and development of unimodal and multimodal biometric systems. {\textcopyright} Springer-Verlag London Limited 2009.},
author = {Fierrez, J. and Galbally, J. and Ortega-Garcia, J. and Freire, M. R. and Alonso-Fernandez, F. and Ramos, D. and Toledano, D. T. and Gonzalez-Rodriguez, J. and Siguenza, J. A. and Garrido-Salas, J. and Anguiano, E. and Gonzalez-de-Rivera, G. and Ribalda, R. and Faundez-Zanuy, M. and Ortega, J. A. and Carde{\~{n}}oso-Payo, V. and Viloria, A. and Vivaracho, C. E. and Moro, Q. I. and Igarza, J. J. and Sanchez, J. and Hernaez, I. and Orrite-Uru{\~{n}}uela, C. and Martinez-Contreras, F. and Gracia-Roche, J. J.},
doi = {10.1007/s10044-009-0151-4},
issn = {14337541},
journal = {Pattern Analysis and Applications},
keywords = {Biometrics,Database,Face,Fingerprint,Hand geometry,Handwriting,Iris,Keystroking,Multimodal,Palmprint,Signature,Speech},
number = {2},
pages = {235--246},
title = {{BiosecurID: A multimodal biometric database}},
volume = {13},
year = {2010}
}
@article{Arsalan2017,
author = {Arsalan, Muhammad and Hong, Hyung Gil and Naqvi, Rizwan Ali and Lee, Min Beom and Kim, Min Cheol and Kim, Dong Seop and Kim, Chan Sik and Park, Kang Ryoung},
doi = {10.3390/sym9110263},
file = {:C$\backslash$:/Users/Shaggy/Desktop/Iris Articles/Deep learning-based iris segmentation for iris recognition in visible light environment.pdf:pdf},
issn = {20738994},
journal = {Symmetry},
keywords = {Biometrics,Convolutional neural network (CNN),Iris recognition,Iris segmentation},
number = {11},
title = {{Deep learning-based iris segmentation for iris recognition in visible light environment}},
volume = {9},
year = {2017}
}
@article{Rattani2017,
abstract = {Ocular biometrics encompasses the imaging and use of characteristic features extracted from the eyes for personal recognition. Ocular biometric modalities in visible light have mainly focused on iris, blood vessel structures over the white of the eye (mostly due to conjunctival and episcleral layers), and periocular region around eye. Most of the existing studies on iris recognition use the near infrared spectrum. However, conjunctival vasculature and periocular regions are imaged in the visible spectrum. Iris recognition in the visible spectrum is possible for light color irides or by utilizing special illumination. Ocular recognition in the visible spectrum is an important research area due to factors such as recognition at a distance, suitability for recognition with regular RGB cameras, and adaptability to mobile devices. Further these ocular modalities can be obtained from a single RGB eye image, and then fused together for enhanced performance of the system. Despite these advantages, the state-of-the-art related to ocular biometrics in visible spectrum is not well known. This paper surveys this topic in terms of computational image enhancement, feature extraction, classification schemes and designed hardware-based acquisition set-ups. Future research directions are also enumerated to identify the path forward.},
annote = {A very detalied survey article. They go through a lot of different approaches to iris recognition. They also talk about the other biometrics that can be extracted from a VIS (Visible Spectrum) image like, moles/freckles/nevi. Other patterns can also b extraxted such as conjunctival vaslulature, periocular and retinal biometrics.

UBIRIS is one of the most used publicaly available databses for VIS iris images with noise.},
author = {Rattani, Ajita and Derakhshani, Reza},
doi = {10.1016/j.imavis.2016.11.019},
file = {:C$\backslash$:/Users/Shaggy/Desktop/Iris Articles/Ocular biometrics in the visible spectrum A survey.pdf:pdf},
issn = {02628856},
journal = {Image and Vision Computing},
keywords = {Biometrics,Conjunctival vasculature,Iris,Mobile biometrics,Ocular biometrics,Periocular biometrics,Visible spectrum},
pages = {1--16},
publisher = {Elsevier B.V.},
title = {{Ocular biometrics in the visible spectrum: A survey}},
url = {http://dx.doi.org/10.1016/j.imavis.2016.11.019},
volume = {59},
year = {2017}
}
@article{lfw2007,
abstract = {Face recognition has benefitted greatly from the many databases that have been produced to study it. Most of these databases have been created under controlled conditions to facilitate the study of specific parameters on the face recognition problem. These parameters include such variables as position, pose, lighting, expression, background, camera quality, occlusion, age, and gender. While there are many applications for face recognition technol- ogy in which one can control the parameters of image acquisition, there are also many applications in which the practitioner has little or no control over such parameters. This database is provided as an aid in studying the latter, unconstrained, face recognition problem. The database represents an initial attempt to provide a set of labeled face photographs spanning the range of conditions typically encountered by people in their everyday lives. The database exhibits natural variability in pose, lighting, focus, resolution, facial expression, age, gender, race, accessories, make-up, occlusions, background, and photographic quality. Despite this variability, the images in the database are presented in a simple and consistent format for maximum ease of use. In addition to describing the details of the database and its acquisition, we provide specific experimental paradigms for which the database is suitable. This is done in an effort to make research performed with the database as consistent and comparable as possible.},
author = {Huang, Gary B and Ramesh, Manu and Berg, Tamara and Learned-Miller, Erik},
doi = {10.1.1.122.8268},
file = {::},
isbn = {9781628414844},
issn = {1996756X},
journal = {University of Massachusetts Amherst Technical Report},
pages = {07--49},
title = {{Labeled faces in the wild: A database for studying face recognition in unconstrained environments}},
url = {http://vis-www.cs.umass.edu/lfw/lfw.pdf},
volume = {1},
year = {2007}
}
@article{Jesus2017,
author = {Jesus, Rosales Banderas Jose De and Maximo, Lopez Sanchez and Raul, Pinto Elias and Gabriel, Gonzalez Serna},
doi = {10.1109/CSCI.2016.0167},
isbn = {9781509055104},
journal = {Proceedings - 2016 International Conference on Computational Science and Computational Intelligence, CSCI 2016},
keywords = {color calibration,image processing,image stabilizer,iris detection},
pages = {861--864},
title = {{Methodology for Iris Scanning through Smartphones}},
year = {2017}
}
@incollection{Bowyer2016,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bowyer, Kevin W and Hollingsworth, Karen P and Flynn, Patrick J},
booktitle = {Handbook of Iris Recognition},
doi = {10.1007/978-1-4471-6784-6},
eprint = {arXiv:1011.1669v3},
file = {::},
isbn = {978-1-4471-6782-2},
issn = {16130073},
pages = {23--61},
pmid = {25246403},
title = {{Chapter 2 A Survey of Iris Biometrics Research: 2008–2010}},
url = {http://link.springer.com/10.1007/978-1-4471-6784-6},
year = {2016}
}
@article{Abate2017,
abstract = {The increasing popularity of smartphones amongst the population laid the basis for a wide range of applications aimed at security and privacy protection. Very modern mobile devices have recently demonstrated the feasibility of using a camera sensor to access the system without typing any alphanumerical password. In this work, we present a method that implements iris recognition in the visible spectrum through unsupervised learning by means of Self Organizing Maps (SOM). The proposed method uses a SOM network to cluster iris features at pixel level. The discriminative feature map is obtained by using RGB data of the iris combined with the statistical descriptors of kurtosis and skewness. An experimental analysis on MICHE-I and UBIRISv1 datasets demonstrates the strengths and weaknesses of the algorithm, which has been specifically designed to require low processing power in compliance with the limited capability of common mobile devices.},
author = {Abate, Andrea F. and Barra, Silvio and Gallo, Luigi and Narducci, Fabio},
doi = {10.1016/j.patrec.2017.02.002},
file = {:C$\backslash$:/Users/Shaggy/Desktop/Iris Articles/Kurtosis and skewness at pixel level as input for SOM networks to iris recognition on mobile devices.pdf:pdf},
isbn = {0000000000},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Iris recognition,Mobile biometric recognition,Statistical descriptors,Unsupervised learning},
pages = {37--43},
pmid = {11341202},
publisher = {Elsevier B.V.},
title = {{Kurtosis and skewness at pixel level as input for SOM networks to iris recognition on mobile devices}},
volume = {91},
year = {2017}
}
@article{Zhao2017a,
annote = {Good article describing a lot of the state of the art deep learning approaches being researched.

Keynotes:

Not much work with deep learning has been done in iris recognition.

THey Use a Fully Convolutional Network (FCN) and talk about others who ahve used a Convolutional Neural Network (CNN). They also mention a Deep Belief Net (DBN) that others have used. DeepIrisNet is also mentioned and tested with

THey have created their own loss function optimized for iris recognition called Extended Triplet Loss (ETL)

Their network is generalizable to other databases meaning that it doesn't require finetuing as many others do.

Used ND-IRIS, CASIA Iris, IITD Iris and WVU Non-Ideal Iris databases to test on.},
author = {Zhao, Zijing and Kumar, Ajay},
doi = {10.1109/ICCV.2017.411},
file = {:C$\backslash$:/Users/Shaggy/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao, Kumar - 2017 - Towards More Accurate Iris Recognition Using Deeply Learned Spatially Corresponding Features.pdf:pdf},
isbn = {978-1-5386-1032-9},
journal = {2017 IEEE International Conference on Computer Vision (ICCV)},
pages = {3829--3838},
title = {{Towards More Accurate Iris Recognition Using Deeply Learned Spatially Corresponding Features}},
url = {http://ieeexplore.ieee.org/document/8237673/},
year = {2017}
}
@article{Zhang2017a,
abstract = {Multimodal classification arises in many computer vi-sion tasks such as object classification and image retrieval. The idea is to utilize multiple sources (modalities) measur-ing the same instance to improve the overall performance compared to using a single source (modality). The varying characteristics exhibited by multiple modalities make it nec-essary to simultaneously learn the corresponding distance metrics. In this paper, we propose a multiple metrics learn-ing algorithm for multimodal data. Metric of each modal-ity is product of two matrices: one matrix is modality spe-cific, the other is enforced to be shared by all the modalities. The learned metrics can improve multimodal classification accuracy and experimental results on four datasets show that the proposed algorithm outperforms existing learning algorithms based on multiple metrics as well as other ap-proaches tested on these datasets. Specifically, we report 95.0{\%} object instance recognition accuracy, 89.2{\%} object category recognition accuracy on the multi-view RGB-D dataset and 52.3{\%} scene category recognition accuracy on SUN RGB-D dataset.},
author = {Zhang, Heng and Patel, Vishal M. and Chellappa, Rama},
doi = {10.1109/CVPR.2017.312},
isbn = {978-1-5386-0457-1},
issn = {1063-6919},
journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {2925--2933},
title = {{Hierarchical Multimodal Metric Learning for Multimodal Classification}},
url = {http://ieeexplore.ieee.org/document/8099795/},
year = {2017}
}
@inproceedings{Yin2011,
abstract = {In this paper, the acquisition and content of a new homologous multimodal biometric database are presented. The SDUMLA-HMT database consists of face images from 7 view angles, finger vein images of 6 fingers, gait videos from 6 view angles, iris images from an iris sensor, and fingerprint images acquired with 5 different sensors. The database includes real multimodal data from 106 individuals. In addition to database description, we also present possible use of the database. The database is available to research community through http://mla.sdu.edu.cn/sdumla-hmt.html .},
author = {Yin, Yilong and Liu, Lili and Sun, Xiwei},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-25449-9_33},
isbn = {9783642254482},
issn = {03029743},
keywords = {Biometrics,Face,Finger vein,Fingerprint,Gait,Homologous,Iris,Multi-modal},
pages = {260--268},
title = {{SDUMLA-HMT: A multimodal biometric database}},
volume = {7098 LNCS},
year = {2011}
}
@article{Sequeira2014,
abstract = {Biometrics represents a return to a natural way of identification: testing someone by what (s)he is, instead of relying on something (s)he owns or knows seems likely to be the way forward. Biometric systems that include multiple sources of information are known as multimodal. Such systems are generally regarded as an alternative to fight a variety of problems all unimodal systems stumble upon. One of the main challenges found in the development of biometric recognition systems is the shortage of publicly available databases acquired under real unconstrained working conditions. Motivated by such need the MobBIO database was created using an Asus EeePad Transformer tablet, with mobile biometric systems in mind. The proposed database is composed by three modalities: iris, face and voice.},
author = {Sequeira, Ana F. and Monteiro, Joao C. and Rebelo, Ana and Oliveira, Helder P.},
isbn = {9789897580093},
journal = {9th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications},
number = {c},
pages = {1--14},
title = {{MobBIO: A Multimodal Database Captured with a Portable Handheld Device}},
year = {2014}
}
@article{Galdi2017a,
author = {Galdi, Chiara and Dugelay, Jean Luc},
doi = {10.1109/ICPR.2016.7899626},
isbn = {9781509048472},
issn = {10514651},
journal = {Proceedings - International Conference on Pattern Recognition},
pages = {160--164},
title = {{Fusing iris colour and texture information for fast iris recognition on mobile devices}},
year = {2017}
}
@article{Nguyen2010,
author = {Nguyen, Kien and Fookes, Clinton and Sridharan, Sridha},
doi = {10.1145/1852611.1852635},
isbn = {9781450301053},
journal = {Proceedings of the 2010 Symposium on Information and Communication Technology - SoICT '10},
keywords = {iris recognition,mbgc,signal-level fusion,super-resolution},
number = {November},
pages = {122},
title = {{Robust mean super-resolution for less cooperative NIR iris recognition at a distance and on the move}},
url = {http://portal.acm.org/citation.cfm?doid=1852611.1852635},
year = {2010}
}
@article{Lee2017,
abstract = {In recent years, the iris recognition system has been gaining increasing acceptance for applications such as access control and smartphone security. When the images of the iris are obtained under unconstrained conditions, an issue of undermined quality is caused by optical and motion blur, off-angle view (the user's eyes looking somewhere else, not into the front of the camera), specular reflection (SR) and other factors. Such noisy iris images increase intra-individual variations and, as a result, reduce the accuracy of iris recognition. A typical iris recognition system requires a near-infrared (NIR) illuminator along with an NIR camera, which are larger and more expensive than fingerprint recognition equipment. Hence, many studies have proposed methods of using iris images captured by a visible light camera without the need for an additional illuminator. In this research, we propose a new recognition method for noisy iris and ocular images by using one iris and two periocular regions, based on three convolutional neural networks (CNNs). Experiments were conducted by using the noisy iris challenge evaluation-part II (NICE.II) training dataset (selected from the university of Beira iris (UBIRIS).v2 database), mobile iris challenge evaluation (MICHE) database, and institute of automation of Chinese academy of sciences (CASIA)-Iris-Distance database. As a result, the method proposed by this study outperformed previous methods.},
author = {Lee, Min Beom and Hong, Hyung Gil and Park, Kang Ryoung},
doi = {10.3390/s17122933},
file = {:C$\backslash$:/Users/Shaggy/Desktop/Iris Articles/Noisy ocular recognition based on three convolutional neural networks.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Convolutional neural network,Iris and periocular,Noisy iris and ocular image},
number = {12},
pmid = {29258217},
title = {{Noisy ocular recognition based on three convolutional neural networks}},
volume = {17},
year = {2017}
}
@article{Khan2017a,
annote = {A good article that uses a new database containing smartphone iris images. Theses are taken in visible light. They use Daugmans approach to localize the iris, then they normalize it. They use wavelets on the image to extract the desired featues and then try to classify them using SVM (97{\%}), KNN (95.1{\%}) and LDA (94.28{\%}).

Keynotes:

A different study has used Sparse Reconstruction Classifier with K-means clustering

A different study obtained 99{\%} accuracy using SVM and Hamming Distance

They tried SVM, K-means, Linear Discrimintant in their own study as classifiers.},
author = {Khan, Fahim Faysal and Akif, Ahnaf and Haque, M A},
isbn = {9781538633748},
pages = {26--28},
title = {{Iris Recognition using Machine Learning from Smartphone Captured Images in Visible Light}},
year = {2017}
}
@inproceedings{Misztal2012,
abstract = {A new method for finding the rotation angle in iris images for biometric identification is presented in this paper. The proposed approach is based on Fourier descriptors analysis and algebraic properties of vector rotation in complex space. {\textcopyright} 2012 IFIP International Federation for Information Processing.},
author = {Misztal, Krzysztof and Tabor, Jacek and Saeed, Khalid},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-33260-9-11},
isbn = {9783642332593},
issn = {03029743},
keywords = {Fourier descriptors,Iris pattern recognition,rotation estimation,rotation recovery},
pages = {135--145},
title = {{A new algorithm for rotation detection in iris pattern recognition}},
volume = {7564 LNCS},
year = {2012}
}
@article{Proenca2017,
abstract = {The effectiveness of current iris recognition systems de-pends on the accurate segmentation and parameterisation of the iris boundaries, as failures at this point misalign the coefficients of the biometric signatures. This paper de-scribes IRINA, an algorithm for Iris Recognition that is ro-bust against INAccurately segmented samples, which makes it a good candidate to work in poor-quality data. The pro-cess is based in the concept of " corresponding " patch be-tween pairs of images, that is used to estimate the posterior probabilities that patches regard the same biological region, even in case of segmentation errors and non-linear texture deformations. Such information enables to infer a free-form deformation field (2D registration vectors) between images, whose first and second-order statistics provide effective bio-metric discriminating power. Extensive experiments were carried out in four datasets (CASIA-IrisV3-Lamp, CASIA-IrisV4-Lamp, CASIA-IrisV4-Thousand and WVU) and show that IRINA not only achieves state-of-the-art performance in good quality data, but also handles effectively severe seg-mentation errors and large differences in pupillary dilation / constriction.},
author = {Proenca, Hugo and Neves, Joao C.},
doi = {10.1109/CVPR.2017.714},
file = {:C$\backslash$:/Users/Shaggy/Desktop/Iris Articles/Proenca{\_}IRINA{\_}Iris{\_}Recognition{\_}CVPR{\_}2017{\_}paper.pdf:pdf},
isbn = {978-1-5386-0457-1},
journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {6747--6756},
title = {{IRINA: Iris Recognition (Even) in Inaccurately Segmented Data}},
url = {http://ieeexplore.ieee.org/document/8100197/},
year = {2017}
}
@article{Arslan2017,
abstract = {Biometric systems may be used to create a remote access model on devices, ensure personal data protection, personalize and facilitate the access security. Biometric systems are generally used to increase the security level in addition to the previous authentication methods and they seen as a good solution. Biometry occupies an important place between the areas of daily life of the machine learning. In this study; the techniques, methods, technologies used in biometric systems are researched, machine learning techniques used biometric aplications are investigated for the security perspective, the advantages and disadvantages that these tecniques provide are given. The studies in the literature between 2010-2016 years, used algorithms, technologies, metrics, usage areas, the machine learning techniques used for different biometric systems such as face, palm prints, iris, voice, fingerprint recognition are researched and the studies made are evaluated. The level of security provided by the use of biometric systems by developed using machine learning and disadvantages that arise in the use of these systems are stated in detail in the study. Also, impact on people of biometric methods in terms of ease of use, security and usages areas are examined.},
author = {Arslan, B. and Yorulmaz, E. and Akca, B. and Sagiroglu, S.},
doi = {10.1109/ICMLA.2016.183},
file = {:C$\backslash$:/Users/Shaggy/Desktop/Iris Articles/Security Perspective of Biometric Recognition and Machine Learning Techniques.pdf:pdf},
isbn = {9781509061662},
journal = {Proceedings - 2016 15th IEEE International Conference on Machine Learning and Applications, ICMLA 2016},
keywords = {Biometric,Face,Fingerprint,Iris,Machine learning,Recognition,Security,Teeth,Voice},
pages = {492--497},
title = {{Security perspective of Biometric recognition and machine learning techniques}},
year = {2017}
}
@article{Ross2003,
annote = {Nice and clear overview of the four basic modules of the standard biometric system.

Very clear explainations of different aspects, but a bit limited in respect to the separation of methods into categories .},
author = {Ross, Arun and Jain, Anil},
isbn = {1517355931},
keywords = {biometrics,decision tree,face,fingerprints,hand geometry,linear discriminant analysis,multimodal,sum rule,verification},
number = {13},
pages = {2115--2125},
title = {{Information Fusion in Biometrics}},
volume = {24},
year = {2003}
}
@article{Al-Waisy2017,
author = {Al-Waisy, Alaa S. and Qahwaji, Rami and Ipson, Stanley and Al-Fahdawi, Shumoos and Nagem, Tarek A.M.},
doi = {10.1007/s10044-017-0656-1},
isbn = {0123456789},
issn = {14337541},
journal = {Pattern Analysis and Applications},
keywords = {AdaGrad method,Convolutional Neural Network,Deep learning,Iris recognition,Multimodal biometric systems,Softmax classifier},
number = {0123456789},
pages = {1--20},
publisher = {Springer London},
title = {{A multi-biometric iris recognition system based on a deep learning approach}},
url = {https://doi.org/10.1007/s10044-017-0656-1},
year = {2017}
}
@article{Chowdhury2016a,
author = {Chowdhury, Anurag and Ghosh, Soumyadeep and Singh, Richa and Vatsa, Mayank},
doi = {10.1109/BTAS.2016.7791199},
isbn = {9781467397339},
journal = {2016 IEEE 8th International Conference on Biometrics Theory, Applications and Systems, BTAS 2016},
title = {{RGB-D face recognition via learning-based reconstruction}},
year = {2016}
}
@article{Petrovska-Delacretaz2008a,
abstract = {Face recognition finds its place in a large number of applications. They occur in different contexts related to security, entertainment or Internet applications. Reliable face recognition is still a great challenge to computer vision and pattern recognition researchers, and new algorithms need to be evaluated on relevant databases. The publicly available IV2 database allows monomodal and multimodal experiments using face data. Known variabilities, that are critical for the performance of the biometric systems (such as pose, expression, illumination and quality) are present. The face and subface data that are acquired in this database are: 2D audio-video talking-face sequences, 2D stereoscopic data acquired with two pairs of synchronized cameras, 3D facial data acquired with a laser scanner, and iris images acquired with a portable infrared camera. The IV2 database is designed for monomodal and multimodal experiments. The quality of the acquired data is of great importance. Therefore as a first step, and in order to better evaluate the quality of the data, a first internal evaluation was conducted. Only a small amount of the total acquired data was used for this evaluation: 2D still images, 3D scans and iris images. First results show the interest of this database. In parallel to the research algorithms, open-source reference systems were also run for baseline comparisons.},
author = {Petrovska-Delacr{\'{e}}taz, D. and Lelandais, S. and Colineau, J. and Chen, L. and Dorizzi, B. and Ardabilian, M. and Krichen, E. and Mellakh, M. A. and Chaari, A. and Guerfi, S. and D'Hose, J. and Amor, B. Ben},
doi = {10.1109/BTAS.2008.4699323},
isbn = {9781424427307},
journal = {BTAS 2008 - IEEE 2nd International Conference on Biometrics: Theory, Applications and Systems},
pages = {3--9},
title = {{The IV2 multimodal biometric database (including Iris, 2D, 3D, stereoscopic, and talking face data), and the IV2-2007 evaluation campaign}},
volume = {00},
year = {2008}
}
@article{Elrefaei2017,
author = {Elrefaei, Lamiaa A. and Hamid, Doaa H. and Bayazed, Afnan A. and Bushnak, Sara S. and Maasher, Shaikhah Y.},
doi = {10.1007/s11042-017-5049-3},
file = {:C$\backslash$:/Users/Shaggy/Desktop/Iris Articles/Developing Iris Recognition System for Smartphone Security.pdf:pdf},
issn = {15737721},
journal = {Multimedia Tools and Applications},
keywords = {Biometrics,Deep Sparse Filter,Hough transform,Iris recognition,Visible Light},
pages = {1--25},
publisher = {Multimedia Tools and Applications},
title = {{Developing Iris Recognition System for Smartphone Security}},
year = {2017}
}
@article{Gopal2018a,
author = {Gopal and Srivastava, Smriti},
doi = {10.1007/s13369-017-2644-6},
issn = {21914281},
journal = {Arabian Journal for Science and Engineering},
keywords = {Feature-level fusion,Multimodal system,Palm–phalanges,Score-level fusion,Unimodal system},
number = {2},
pages = {543--554},
publisher = {Springer Berlin Heidelberg},
title = {{Accurate Human Recognition by Score-Level and Feature-Level Fusion Using Palm–Phalanges Print}},
volume = {43},
year = {2018}
}
@article{Mellakh2009a,
author = {Mellakh, A and Chaari, A and Guerfi, S and Dhose, J and Colineau, J and Lelandais, S and Petrovska-Delacr{\`{e}}taz, D and Dorizzi, B},
doi = {10.1007/978-3-642-04697-1_3},
isbn = {03029743; 3642046967 (ISBN); 9783642046964 (ISBN)},
issn = {03029743},
journal = {11th International Conference on Advanced Concepts for Intelligent Vision Systems, ACIVS 2009},
keywords = {2D face recognition,Appearance based,Appearance-based algorithms,Computer vision,Database,Database systems,Discriminant analysis,Evaluation campaign,Experimental protocols,Face images,Face recognition,Linear discriminant analysis,Multi-modal,Multimodal database,Principal component analysis,Training sets},
pages = {24--32},
title = {{2D face recognition in the IV2 evaluation campaign}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-70549098348{\&}partnerID=40{\&}md5=0541be27bb29a037c965f0d644b15856},
volume = {5807 LNCS},
year = {2009}
}
@article{Schiel2002,
abstract = {In this contribution we announce and describe in detail the new multimodal corpus evolving from the publicly funded German SmartKom project. The first release of the corpus (BAS SK-P 1.0) has been finished end of 2001 and will be ready for distribution to the scientific community in July 2002. The SmartKom corpus will be the first of a new generation of Language Resources (LR) designed for a more or less complete data gathering of human-machine communication combining acoustic, visual and tactile input and output modalities. Since the funding of about EU 2 Mio for this LR is 100{\%} public, the corpus will be available without royalties via the Bavarian Archive for Speech Signals (BAS) at the University of Munich.},
author = {Schiel, Florian and Steininger, Silke and T{\"{u}}rk, Ulrich},
journal = {Proceedings of the 3rd Language Resources and Evaluation},
number = {34},
pages = {200--2006},
title = {{The SmartKom Multimodal Corpus at BAS}},
year = {2002}
}
@article{Biosec2007,
abstract = {The baseline corpus of a new multimodal database, acquired in the framework of the FP6 EU BioSec Integrated Project, is presented. The corpus consists of fingerprint images acquired with three different sensors, frontal face images from a webcam, iris images from an iris sensor, and voice utterances acquired both with a close-talk headset and a distant webcam microphone. The BioSec baseline corpus includes real multimodal data from 200 individuals in two acquisition sessions. In this contribution, the acquisition setup and protocol are outlined, and the contents of the corpus-including data and population statistics-are described. The database will be publicly available for research purposes by mid-2006. {\textcopyright} 2006 Pattern Recognition Society.},
author = {Fierrez, Julian and Ortega-Garcia, Javier and {Torre Toledano}, Doroteo and Gonzalez-Rodriguez, Joaquin},
doi = {10.1016/j.patcog.2006.10.014},
isbn = {0031-3203},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Authentication,Biometrics,Database,Face,Fingerprint,Iris,Multimodal,Performance,Verification,Voice},
number = {4},
pages = {1389--1392},
title = {{Biosec baseline corpus: A multimodal biometric database}},
volume = {40},
year = {2007}
}
@article{DeMarsico2018,
abstract = {Mobile biometrics technologies are nowadays the new frontier for secure use of data and services, and are considered particularly important due to the massive use of handheld devices in the entire world. Among the biometric traits with potential to be used in mobile settings, the iris/ocular region is a natural candidate, even considering that further advances in the technology are required to meet the operational requirements of such ambitious environments. Aiming at promoting these advances, we organized the Mobile Iris Challenge Evaluation (MICHE)-I contest. This paper presents a comparison of the performance of the participant methods by various Figures of Merit (FoMs). A particular attention is devoted to the identification of the image covariates that are likely to cause a decrease in the performance levels of the compared algorithms. Among these factors, interoperability among different devices plays an important role. The methods (or parts of them) implemented by the analyzed approaches are classified into segmentation (S), which was the main target of MICHE-I, and recognition (R). The paper reports both the results observed for either S or R, and also for different recombinations (S+R) of such methods. Last but not least, we also present the results obtained by multi-classifier strategies.},
author = {{De Marsico}, Maria and Nappi, Michele and Narducci, Fabio and Proen{\c{c}}a, Hugo},
doi = {10.1016/j.patcog.2017.08.028},
file = {:C$\backslash$:/Users/Shaggy/Desktop/Iris Articles/Insights into the results of MICHE I - Mobile Iris CHallenge Evaluation 2018.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Biometric algorithm fusion,Evaluation,Mobile Iris Recognition},
pages = {286--304},
publisher = {Elsevier Ltd},
title = {{Insights into the results of MICHE I - Mobile Iris CHallenge Evaluation}},
volume = {74},
year = {2018}
}
@article{Luhadiya2017,
annote = {Really good article with a summery of Iris recognition history and approaches in the introduction part.


Keynotes:

Iris database called CASIA with 756 images of 108 people.

SVM used to classify irises.

Elman Recurrent Neural Netowrk used.},
author = {Luhadiya, Ruchi and Khedkar, Anagha},
doi = {10.1109/ICAECCT.2016.7942619},
file = {:C$\backslash$:/Users/Shaggy/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Luhadiya, Khedkar - 2017 - Iris detection for person identification using multiclass SVM.pdf:pdf},
isbn = {9781509036622},
journal = {2016 IEEE International Conference on Advances in Electronics, Communication and Computer Technology, ICAECCT 2016},
keywords = {GLCM,Hough circular transform,Iris,Machine learning,Person identification,SVM},
pages = {387--392},
title = {{Iris detection for person identification using multiclass SVM}},
year = {2017}
}
@article{Daugman1993,
author = {Daugman, J.G.},
doi = {10.1109/34.244676},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {11},
pages = {1148--1161},
title = {{High confidence visual recognition of persons by a test of statistical independence}},
url = {http://ieeexplore.ieee.org/document/244676/},
volume = {15},
year = {1993}
}
@article{Chen2018,
abstract = {In this work, we address the problem of face verification, namely determining whether a pair of face images belongs to the same or different subjects. Previous works often consider solving the problem of face verification in two steps: feature extraction and face recognition, resulting in a fragmented procedure. We argue that these techniques, although working well, fail to explicitly exploit a full end-to-end framework for face verification, which has received much attention and achieved significant improvements recently. In this paper, we propose a novel Joint Bayesian guided metric learning technique for dealing with the face verification task, which well integrates the above two steps of face verification into an end-to-end convolutional neural network (CNN) architecture. In the training stage, an initial neural network, which has the similar architecture with GoogLeNet CNN model, is firstly pre-trained by optimizing classification-based objective functions on the publicly available CASIA WebFace database. Based on constructed face pairs dataset from CASIA WebFace and LFW datasets, we then fine-tune the whole network parameters under the guide of the learned knowledge, which is obtained from the highly successful Joint Bayesian model. This guided learning procedure, which can also be seen as a metric learning technique, can further update network parameters for discriminating face pairs. In the testing process, the outputs by this unified network are discriminated with a threshold value to produce the ultimate prediction for the face verification task. Comprehensive evaluations over the LFW dataset well demonstrate the encouraging face verification performance of our proposed framework.},
author = {Chen, Di and Xu, Chunyan and Yang, Jian and Qian, Jianjun and Zheng, Yuhui and Shen, Linlin},
doi = {10.1016/J.NEUCOM.2017.09.009},
file = {::},
issn = {0925-2312},
journal = {Neurocomputing},
month = {jan},
pages = {560--567},
publisher = {Elsevier},
title = {{Joint Bayesian guided metric learning for end-to-end face verification}},
url = {https://www-sciencedirect-com.zorac.aub.aau.dk/science/article/pii/S0925231217314807{\#}bib0016},
volume = {275},
year = {2018}
}
@article{Fierrez2018b,
abstract = {We provide an introduction to Multiple Classifier Systems (MCS) including basic nomenclature and describing key elements: classifier dependencies, type of classifier outputs, aggregation procedures, architecture, and types of methods. This introduction complements other existing overviews of MCS, as here we also review the most prevalent theoretical framework for MCS and discuss theoretical developments related to MCS. The introduction to MCS is then followed by a review of the application of MCS to the particular field of multimodal biometric person authentication in the last 25 years, as a prototypical area in which MCS has resulted in important achievements. This review includes general descriptions of successful MCS methods and architectures in order to facilitate the export of them to other information fusion problems. Based on the theory and framework introduced here, in the companion paper we then develop in more technical detail recent trends and developments in MCS from multimodal biometrics that incorporate context information in an adaptive way. These new MCS architectures exploit input quality measures and pattern-specific particularities that move apart from general population statistics, resulting in robust multimodal biometric systems. Similarly as in the present paper, methods in the companion paper are introduced in a general way so they can be applied to other information fusion problems as well. Finally, also in the companion paper, we discuss open challenges in biometrics and the role of MCS to advance them.},
author = {Fierrez, Julian and Morales, Aythami and Vera-Rodriguez, Ruben and Camacho, David},
doi = {10.1016/j.inffus.2017.12.003},
issn = {15662535},
journal = {Information Fusion},
keywords = {Adaptive,Biometrics,Classifier,Context,Fusion,Multimodal},
number = {November 2017},
pages = {57--64},
publisher = {Elsevier},
title = {{Multiple classifiers in biometrics. part 1: Fundamentals and review}},
url = {https://doi.org/10.1016/j.inffus.2017.12.003},
volume = {44},
year = {2018}
}
@article{Zhang2016,
author = {Zhang, Man and Zhang, Qi and Sun, Zhenan and Zhou, Shujuan and Ahmed, Nasir Uddin},
doi = {10.1109/BTAS.2016.7791191},
file = {:C$\backslash$:/Users/Shaggy/Desktop/Iris Articles/The BTAS∗Competition on Mobile Iris Recognition.pdf:pdf},
isbn = {9781467397339},
journal = {2016 IEEE 8th International Conference on Biometrics Theory, Applications and Systems, BTAS 2016},
title = {{The BTAS∗Competition on Mobile Iris Recognition}},
year = {2016}
}
@article{Cheng2017a,
abstract = {Sea–land segmentation and ship detection are two prevalent research domains for optical remote sensing harbor images and can find many applications in harbor supervision and management. As the spatial resolution of imaging technology improves, traditional methods struggle to perform well due to the complicated appearance and background distributions. In this paper, we unify the above two tasks into a single framework and apply the deep convolutional neural networks to predict pixelwise label for an input. Specifically, an edge aware convolutional network is proposed to parse a remote sensing harbor image into three typical objects, e.g., sea, land, and ship. Two innovations are made on top of the deep structure. First, we design a multitask model by simultaneously training the segmentation and edge detection networks. Hierarchical semantic features from the segmentation network are extracted to learn the edge network. Second, the outputs of edge pipeline are further employed to refine entire model by adding an edge aware regularization, which helps our method to yield very desirable results that are spatially consistent and well boundary located. It also benefits the segmentation of docked ships that are quite challenging for many previous methods. Experimental results on two datasets collected from Google Earth have demonstrated the effectiveness of our approach both in quantitative and qualitative performance compared with state-of-the-art methods.},
author = {Cheng, Dongcai and Meng, Gaofeng and Xiang, Shiming and Pan, Chunhong},
doi = {10.1109/JSTARS.2017.2747599},
issn = {21511535},
journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
keywords = {Edge aware regularization,harbor images,multitask learning,semantic segmentation},
number = {12},
pages = {5769--5783},
title = {{FusionNet: Edge Aware Deep Convolutional Networks for Semantic Segmentation of Remote Sensing Harbor Images}},
volume = {10},
year = {2017}
}
@article{Kim2016,
abstract = {The iris recognition on a mobile phone is different from the conventional iris recognition implemented on a dedicated device in that the computational power of a mobile phone and the space for placing NIR (near infrared) LED (light emitting diode) illuminators and iris camera are limited. This paper raises these issues in detail based on real implementation of an iris recognition system in a mobile phone and proposes some solutions to these issues. An experimental study was conducted to search for the relevant power and wavelength of NIR LED illuminators with their positioning on a phone for capturing a good quality iris image. Subsequently, in view of the disparity between the user's gazing point and the center of the iris camera which causes degradation of acquired iris images, an experiment was performed to locate the appropriate gazing point for good iris image capture. A fast eye detection algorithm was proposed for implementation under the mobile platform with low computational facility. The experiments were conducted on a currently released mobile phone and the results showed promising potential for adoption of iris recognition as a reliable authentication means. As a result, two 850 nm LEDs were selected for iris illumination at 1.1 cm away from the iris camera for the size of a 7 cm × 13.7 cm phone. In the performance, the recognition accuracy was 0.1{\%} EER (equal error rate) and the eye detection rate with the speed of 17.64 ms on a mobile phone was 99.4{\%}.},
annote = {Keynotes:
Contributes with a good NIR mobile algorithm that's fast on mobile phones. They also contribute with an mobile iris database of 500 images that they are willing to share for research. Could not find it online.},
author = {Kim, Dongik and Jung, Yujin and Toh, Kar Ann and Son, Byungjun and Kim, Jaihie},
doi = {10.1016/j.eswa.2016.01.050},
file = {:C$\backslash$:/Users/Shaggy/Desktop/Iris Articles/An empirical study on iris recognition in a mobile phone.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Handheld,Iris recognition,Mobile,Portable,Smartphone},
pages = {328--339},
publisher = {Elsevier Ltd},
title = {{An empirical study on iris recognition in a mobile phone}},
url = {http://dx.doi.org/10.1016/j.eswa.2016.01.050},
volume = {54},
year = {2016}
}
@article{Daugman2009,
abstract = {This chapter explains the iris recognition algorithms and presents results of 9.1 million comparisons among eye images from trials in Britain, the USA, Japan, and Korea. The key to iris recognition is the failure of a test of statistical independence, which involves so many degrees-of-freedom that this test is virtually guaranteed to be passed whenever the phase codes for two different eyes are compared, but to be uniquely failed when any eye's phase code is compared with another version of itself. The test of statistical independence is implemented by the simple Boolean Exclusive-OR operator (XOR) applied to the 2048 bit phase vectors that encode any two iris patterns, masked (AND'ed) by both of their corresponding mask bit vectors to prevent non iris artifacts from influencing iris comparisons. The XOR operator detects disagreement between any corresponding pair of bits, while the AND operator ensures that the compared bits are both deemed to have been uncorrupted by eyelashes, eyelids, specular reflections, or other noise. The norms of the resultant bit vector and of theAND'ed mask vectors are then measured in order to compute a fractional Hamming Distance as the measure of the dissimilarity between any two irises, whose two phase code bit vectors are denoted {\{}. codeA, codeB{\}} and whose mask bit vectors are denoted {\{}. maskA, maskB{\}}. {\textcopyright} 2009 Elsevier Inc. All rights reserved.},
annote = {A chapter about how Iris Recognitions works in general. John Daugman is the creator of IrisCode, a 2D Gabor wavelet-based iris recognition algorithm that is the basis of all publicly deployed automatic iris recognition systems and which has registered more than a billion persons worldwide in government ID programs.

Keywords/phrases:
Near Infra Red (NIR) images can be used for iris capturing. 

Gabor wavelets are used for determining the inter and outer edges of an iris.

Often the iris will not be circular because an eyelid will cover it.

Hamming distance is used when comparing two irises.},
author = {Daugman, John},
doi = {10.1016/B978-0-12-374457-9.00025-1},
file = {:C$\backslash$:/Users/Shaggy/Desktop/Iris Articles/The{\_}Essential{\_}Guide{\_}to{\_}Image{\_}Processing{\_}----{\_}(Chapter{\_}25.{\_}How{\_}Iris{\_}Recognition{\_}Works).pdf:pdf},
isbn = {9780123744579},
issn = {10518215},
journal = {The Essential Guide to Image Processing},
pages = {715--739},
pmid = {20810146},
title = {{How Iris Recognition Works}},
year = {2009}
}
@article{Lv2015,
abstract = {There is an increasing interest in creating pervasive games based on emerging interaction technologies. In order to develop touch-less, interactive and augmented reality games on vision-based wearable device, a touch-less motion interaction technology is designed and evaluated in this work. Users interact with the augmented reality games with dynamic hands/feet gestures in front of the camera, which triggers the interaction event to interact with the virtual object in the scene. Three primitive augmented reality games with eleven dynamic gestures are developed based on the proposed touch-less interaction technology as proof. At last, a comparing evaluation is proposed to demonstrate the social acceptability and usability of the touch-less approach, running on a hybrid wearable framework or with Google Glass, as well as workload assessment, user's emotions and satisfaction.},
author = {Lv, Zhihan and Halawani, Alaa and Feng, Shengzhong and {Ur R{\'{e}}hman}, Shafiq and Li, Haibo},
doi = {10.1007/s00779-015-0844-1},
isbn = {16174909 (ISSN)},
issn = {16174909},
journal = {Personal and Ubiquitous Computing},
keywords = {Augmented reality game,Hand free,Pervasive game,Smartphone game,Touch-less,Wearable device},
number = {3},
pages = {551--567},
title = {{Touch-less interactive augmented reality game on vision-based wearable device}},
volume = {19},
year = {2015}
}
@article{Ortega-Garcia2010,
abstract = {A new multimodal biometric database designed and acquired within the framework of the European BioSecure Network of Excellence is presented. It is comprised of more than 600 individuals acquired simultaneously in three scenarios: 1) over the Internet, 2) in an office environment with desktop PC, and 3) in indoor/outdoor environments with mobile portable hardware. The three scenarios include a common part of audio/video data. Also, signature and fingerprint data have been acquired both with desktop PC and mobile portable hardware. Additionally, hand and iris data were acquired in the second scenario using desktop PC. Acquisition has been conducted by 11 European institutions. Additional features of the BioSecure Multimodal Database (BMDB) are: two acquisition sessions, several sensors in certain modalities, balanced gender and age distributions, multimodal realistic scenarios with simple and quick tasks per modality, cross-European diversity, availability of demographic data, and compatibility with other multimodal databases. The novel acquisition conditions of the BMDB allow us to perform new challenging research and evaluation of either monomodal or multimodal biometric systems, as in the recent BioSecure Multimodal Evaluation campaign. A description of this campaign including baseline results of individual modalities from the new database is also given. The database is expected to be available for research purposes through the BioSecure Association during 2008.},
author = {Ortega-Garcia, Javier and Fierrez, Julian and Alonso-Fernandez, Fernando and Galbally, Javier and Freire, Manuel R. and Gonzalez-Rodriguez, Joaquin and Garcia-Mateo, Carmen and Alba-Castro, Jose Luis and Gonzalez-Agulla, Elisardo and Otero-Muras, Enrique and Garcia-Salicetti, Sonia and Allano, Lorene and Ly-Van, Bao and Dorizzi, Bernadette and Kittler, Josef and Bourlai, Thirimachos and Poh, Norman and Deravi, Farzin and Ng, Ming N.R. and Fairhurst, Michael and Hennebert, Jean and Humm, Andreas and Tistarelli, Massimo and Brodo, Linda and Richiardi, Jonas and Drygajlo, Andrezj and Ganster, Harald and Sukno, Federico M. and Pavani, Sri Kaushik and Frangi, Alejandro and Akarun, Lale and Savran, Arman},
doi = {10.1109/TPAMI.2009.76},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Benchmark,Biometrics,Database,Evaluation,Face,Fingerprint,Hand,Iris.,Multimodal,Performance,Signature,Speaker,Voice},
number = {6},
pages = {1097--1111},
pmid = {20431134},
title = {{The multiscenario multienvironment biosecure multimodal database (BMDB)}},
volume = {32},
year = {2010}
}
@book{Wechsler2007,
abstract = {One of the grand challenges for computational intelligence and biometrics is to understand how people process and recognize faces and to develop automated and reliable face recognition systems. Biometrics has become the major component in the complex decision making process associated with security applications. The face detection and authentication challenges addressed include cluttered environments, image variability, occlusion and disguise, and temporal changes all within open set recognition. Reliable Face Recognition Methods: System Design, Implementation and Evaluation comprehensively explores the face recognition problem while drawing inspiration from complementary disciplines such as neurosciences, statistics, signal and image processing, computer vision, and machine learning and pattern recognition. This book also examines the evolution of face recognition research and explores promising new avenues for research and development. Reliable Face Recognition Methods: System Design, Implementation and Evaluation benefits graduate-level students, researchers, and practitioners, as well as government and industry decision makers in the security arena. Ruud Bolle (IBM): "Harry Wechsler's monograph provides a thorough, up-to-date, and in-depth overview of the many advances in the area of face recognition. gives an excellent overview of the issues related to security and privacy when it comes to automated biometrics. In summary, this book has the potential to become a classic. Harry Wechsler is to be commended for undertaking the monumental task of writing this book." John Daugman (Cambridge University, UK): "The book looks excellent. The topic is timely, and the perspective-multi-disciplinary, measured, and objective - is much needed and welcome. I believe that Wechsler's latest book will make a valued contribution to this important field and will become a standard." David Zhang (Hong Kong Polytechnic University, China): "From a system view, this book shows an excellent arrangement, from data collection to face recognition, as well as system performance evaluation and error analysis. This book can serve both as an interdisciplinary text and a research reference. Each chapter provides the background and impetus for understanding the problems discussed." Stan Li (Chinese Academy of Sciences, China): "The book treats the subject of face recognition in a fairly systematic way. The book covers most relevant topics in face recognition, in a well organized way. The information is also useful for experts. The manuscript is easy to read." Tom Huang (University of Illinois, USA): "Harry Wechsler is without doubt one of the leading authorities in face recognition and related topics. We have recently seen a number of excellent edited books on Face Recognition. However, Wechsler's book is the first unified treatment of this important subject. In my opinion, Wechsler is one of only a handful of people in the world who could write such a comprehensive, unified, informative, perceptive, and authoritative book on Face Recognition. Harry Wechsler so far, is the only one of this handful who took the time and effort to realize such a project. The book certainly will have a great positive impact on the biometrics research community. But also, because it looks at Face Recognition from different perspectives, it will be welcomed by non-experts." {\textcopyright} 2007 Springer Science+Business Media, LLC. All rights reserved.},
address = {Boston, MA},
author = {Wechsler, Harry},
booktitle = {Reliable Face Recognition Methods: System Design, Impementation and Evaluation},
doi = {10.1007/978-0-387-38464-1},
file = {::},
isbn = {038722372X},
pages = {1--329},
publisher = {Springer US},
title = {{Reliable face recognition methods: System design, implementation and evaluation}},
url = {http://link.springer.com/10.1007/978-0-387-38464-1 https://link-springer-com.zorac.aub.aau.dk/book/10.1007{\%}2F978-0-387-38464-1{\#}about},
year = {2007}
}
@article{Ribeiro2017,
author = {Ribeiro, Eduardo and Uhl, Andreas and Alonso-Fernandez, Fernando and Farrugia, Reuben A.},
doi = {10.23919/EUSIPCO.2017.8081595},
file = {::},
isbn = {978-0-9928626-7-1},
journal = {2017 25th European Signal Processing Conference (EUSIPCO)},
pages = {2176--2180},
title = {{Exploring deep learning image super-resolution for iris recognition}},
url = {http://ieeexplore.ieee.org/document/8081595/},
volume = {2},
year = {2017}
}
@article{Kumar2016a,
abstract = {We studied the fusion of three biometric authentication modalities, namely, swiping gestures, typing patterns and the phone movement patterns observed during typing or swiping. A web browser was customized to collect the data generated from the aforementioned modalities over four to seven days in an unconstrained environment. Several features were extracted by using sliding window mechanism for each modality and analyzed by using information gain, correlation, and symmetric uncertainty. Finally, five features from windows of continuous swipes, thirty features from windows of continuously typed letters, and nine features from corresponding phone movement patterns while swiping/typing were used to build the authentication system. We evaluated the performance of each modality and their fusion over a dataset of 28 users. The feature-level fusion of swiping and the corresponding phone movement patterns achieved an authentication accuracy of 93.33{\%}, whereas, the score-level fusion of typing behaviors and the corresponding phone movement patterns achieved an authentication accuracy of 89.31{\%}. 1.},
author = {Kumar, Rajesh and Phoha, Vir V. and Serwadda, Abdul},
doi = {10.1109/BTAS.2016.7791164},
isbn = {9781467397339},
journal = {2016 IEEE 8th International Conference on Biometrics Theory, Applications and Systems, BTAS 2016},
title = {{Continuous authentication of smartphone users by fusing typing, swiping, and phone movement patterns}},
year = {2016}
}
@article{Bazrafkan2017,
abstract = {With the increasing imaging and processing capabilities of today's mobile devices, user authentication using iris biometrics has become feasible. However, as the acquisition conditions become more unconstrained and as image quality is typically lower than dedicated iris acquisition systems, the accurate segmentation of iris regions is crucial for these devices. In this work, an end to end Fully Convolutional Deep Neural Network (FCDNN) design is proposed to perform the iris segmentation task for lower-quality iris images. The network design process is explained in detail, and the resulting network is trained and tuned using several large public iris datasets. A set of methods to generate and augment suitable lower quality iris images from the high-quality public databases are provided. The network is trained on Near InfraRed (NIR) images initially and later tuned on additional datasets derived from visible images. Comprehensive inter-database comparisons are provided together with results from a selection of experiments detailing the effects of different tunings of the network. Finally, the proposed model is compared with SegNet-basic, and a near-optimal tuning of the network is compared to a selection of other state-of-art iris segmentation algorithms. The results show very promising performance from the optimized Deep Neural Networks design when compared with state-of-art techniques applied to the same lower quality datasets.},
archivePrefix = {arXiv},
arxivId = {1712.02877},
author = {Bazrafkan, Shabab and Thavalengal, Shejin and Corcoran, Peter},
eprint = {1712.02877},
file = {:C$\backslash$:/Users/Shaggy/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bazrafkan, Thavalengal, Corcoran - 2017 - An End to End Deep Neural Network for Iris Segmentation in Unconstraint Scenarios.pdf:pdf},
month = {dec},
title = {{An End to End Deep Neural Network for Iris Segmentation in Unconstraint Scenarios}},
url = {http://arxiv.org/abs/1712.02877},
year = {2017}
}
@article{Suk2014,
abstract = {For the last decade, it has been shown that neuroimaging can be a potential tool for the diagnosis of Alzheimer's Disease (AD) and its prodromal stage, Mild Cognitive Impairment (MCI), and also fusion of different modalities can further provide the complementary information to enhance diagnostic accuracy. Here, we focus on the problems of both feature representation and fusion of multimodal information from Magnetic Resonance Imaging (MRI) and Positron Emission Tomography (PET). To our best knowledge, the previous methods in the literature mostly used hand-crafted features such as cortical thickness, gray matter densities from MRI, or voxel intensities from PET, and then combined these multimodal features by simply concatenating into a long vector or transforming into a higher-dimensional kernel space. In this paper, we propose a novel method for a high-level latent and shared feature representation from neuroimaging modalities via deep learning. Specifically, we use Deep Boltzmann Machine (DBM).22Although it is clear from the context that the acronym DBM denotes "Deep Boltzmann Machine" in this paper, we would clearly indicate that DBM here is not related to "Deformation Based Morphometry"., a deep network with a restricted Boltzmann machine as a building block, to find a latent hierarchical feature representation from a 3D patch, and then devise a systematic method for a joint feature representation from the paired patches of MRI and PET with a multimodal DBM. To validate the effectiveness of the proposed method, we performed experiments on ADNI dataset and compared with the state-of-the-art methods. In three binary classification problems of AD vs. healthy Normal Control (NC), MCI vs. NC, and MCI converter vs. MCI non-converter, we obtained the maximal accuracies of 95.35{\%}, 85.67{\%}, and 74.58{\%}, respectively, outperforming the competing methods. By visual inspection of the trained model, we observed that the proposed method could hierarchically discover the complex latent patterns inherent in both MRI and PET.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Suk, Heung Il and Lee, Seong Whan and Shen, Dinggang},
doi = {10.1016/j.neuroimage.2014.06.077},
eprint = {NIHMS150003},
isbn = {1053-8119},
issn = {10959572},
journal = {NeuroImage},
keywords = {Alzheimer's Disease,Deep boltzmann machine,Mild cognitive impairment,Multimodal data fusion,Shared feature representation},
pages = {569--582},
pmid = {25042445},
publisher = {Elsevier Inc.},
title = {{Hierarchical feature representation and multimodal fusion with deep learning for AD/MCI diagnosis}},
url = {http://dx.doi.org/10.1016/j.neuroimage.2014.06.077},
volume = {101},
year = {2014}
}
@article{Zapata2017a,
author = {Zapata, J C and Duque, C M and Gonzalez, M E},
doi = {10.1007/978-981-10-5427-3},
isbn = {978-981-10-5426-6},
keywords = {biometric {\'{a}} data fusion,signals {\'{a}} signal processing},
number = {i},
pages = {721--733},
title = {{Advances in Computing and Data Sciences}},
url = {http://link.springer.com/10.1007/978-981-10-5427-3},
volume = {721},
year = {2017}
}
@article{Krizhevsky2017,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0 {\%} which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3{\%}, compared to 26.2 {\%} achieved by the second-best entry. 1},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
doi = {10.1145/3065386},
eprint = {1102.0183},
isbn = {9781627480031},
issn = {00010782},
journal = {Communications of the ACM},
month = {may},
number = {6},
pages = {84--90},
pmid = {7491034},
publisher = {ACM},
title = {{ImageNet classification with deep convolutional neural networks}},
url = {http://dl.acm.org/citation.cfm?doid=3098997.3065386},
volume = {60},
year = {2017}
}
@article{Neves2017,
abstract = {{\textcopyright} 2017 IEEE. An error-correcting code (ECC) is a process of adding redundant data to a message, such that it can be recovered by a receiver even if a number of errors are introduced in transmission. Inspired by the principles of ECC, we introduce a method capable of detecting degraded features in biometric signatures by exploiting feature correlation. The main novelty is that, unlike existing biometric cryptosystems, the proposed method works directly on the biometric signature. Our approach performs a redundancy analysis of non-degraded data to build an undirected graphical model (Markov Random Field), whose energy minimization determines the sequence of degraded components of the biometric sample. Experiments carried out in different biometric traits ascertain the improvements attained when disregarding degraded features during the matching phase. Also, we stress that the proposed method is general enough to work in different classification methods, such as CNNs.},
author = {Neves, Joao and Proenca, Hugo},
doi = {10.1109/FG.2017.122},
file = {:C$\backslash$:/Users/Shaggy/Desktop/Iris Articles/Exploiting Data Redundancy for Error Detection in Degraded Biometric Signatures Resulting From in the Wild Environments.pdf:pdf},
isbn = {9781509040230},
journal = {Proceedings - 12th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2017 - 1st International Workshop on Adaptive Shot Learning for Gesture Understanding and Production, ASL4GUP 2017, Biometrics in the Wild, Bwild 2017, Heteroge},
pages = {981--986},
title = {{Exploiting Data Redundancy for Error Detection in Degraded Biometric Signatures Resulting from in the Wild Environments}},
year = {2017}
}
@incollection{Bowyer2016b,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
annote = {Read only chapter 17 about iris and face fusion. The article gives a nice and clean overview of different multi-biometric systems as well as levels of data abstraction the data fusion can be applied on. 

The work presented performs iris and face fusion using multi-sample, multi instance, and multimodal data. it fuses the multi sample, and multi instance together, by simply finding the best match in the variations. this is done as a score level fusion. and it fuses the two modalities by rank-level fusion using Broda count.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Connaughton, Ryan and Bowyer, Kevin W and Flynn, Patrick J},
booktitle = {Handbook of Iris Recognition},
doi = {10.1007/978-1-4471-6784-6},
eprint = {arXiv:1011.1669v3},
isbn = {978-1-4471-6782-2},
issn = {16130073},
pages = {397--415},
pmid = {25246403},
title = {{Chapter 17 Fusion of Face and Iris Biometrics}},
url = {http://link.springer.com/10.1007/978-1-4471-6784-6},
year = {2016}
}
@article{Ho1994,
abstract = {A multiple classifier system is a powerful solution to difficult pattern$\backslash$nrecognition problems involving large class sets and noisy input$\backslash$nbecause it allows simultaneous use of arbitrary feature descriptors$\backslash$nand classification procedures. Decisions by the classifiers can$\backslash$nbe represented as rankings of classifiers and different instances$\backslash$nof a problem. The rankings can be combined by methods that either$\backslash$nreduce or rerank a given set of classes. An intersection method$\backslash$nand union method are proposed for class set reduction. Three methods$\backslash$nbased on the highest rank, the Borda count, and logistic regression$\backslash$nare proposed for class set reranking. These methods have been tested$\backslash$nin applications of degraded machine-printed characters and works$\backslash$nfrom large lexicons, resulting in substantial improvement in overall$\backslash$ncorrectness.},
author = {Ho, Tin Kam and Hull, Jonathan J and Srihari, Sargur N},
doi = {http://dx.doi.org/10.1109/34.273716},
isbn = {0162-8828 VO - 16},
issn = {0162-8828},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
number = {1},
pages = {66--75},
title = {{Decision Combination in Multiple Classifier Systems}},
volume = {16},
year = {1994}
}
@inproceedings{Sun2014,
abstract = {The key challenge of face recognition is to develop effective feature representations for reducing intra-personal variations while enlarging inter-personal differences. In this paper, we show that it can be well solved with deep learning and using both face identification and verification signals as supervision. The Deep IDentification-verification features (DeepID2) are learned with carefully designed deep convolutional networks. The face identification task increases the inter-personal variations by drawing DeepID2 extracted from different identities apart, while the face verification task reduces the intra-personal variations by pulling DeepID2 extracted from the same identity together, both of which are essential to face recognition. The learned DeepID2 features can be well generalized to new identities unseen in the training data. On the challenging LFW dataset, 99.15{\%} face verification accuracy is achieved. Compared with the best deep learning result on LFW, the error rate has been significantly reduced by 67{\%}.},
archivePrefix = {arXiv},
arxivId = {1406.4773},
author = {Sun, Yi and Wang, Xiaogang and Tang, Xiaoou},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2014.244},
eprint = {1406.4773},
file = {::},
isbn = {9781479951178},
issn = {10636919},
keywords = {deep learning,face verification},
pages = {1891--1898},
pmid = {21808091},
title = {{Deep Learning Face Representation by Joint Identification-Verification}},
url = {https://arxiv.org/pdf/1406.4773.pdf http://arxiv.org/abs/1406.4773},
year = {2014}
}
@article{Simonyan2015,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
doi = {10.1016/j.infsof.2008.09.005},
eprint = {1409.1556},
file = {::},
isbn = {9781450341448},
issn = {09505849},
journal = {International Conference on Learning Representations (ICRL)},
month = {sep},
pages = {1--14},
pmid = {16873662},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://arxiv.org/abs/1409.1556},
year = {2015}
}
@article{Aakerberg2017,
abstract = {Object recognition is one of the important tasks in computer vision which has found enormous applications. Depth modality is proven to provide supplementary information to the common RGB modality for object recognition. In this paper, we propose methods to improve the recognition performance of an existing deep learning based RGB-D object recognition model, namely the FusionNet proposed by Eitel et al. First, we show that encoding the depth values as colorized surface normals is beneficial, when the model is initialized with weights learned from training on ImageNet data. Additionally, we show that the RGB stream of the FusionNet model can benefit from using deeper network architectures, namely the 16-layered VGGNet, in exchange for the 8-layered CaffeNet. In combination, these changes improves the recognition performance with 2.2{\%} in comparison to the original FusionNet, when evaluating on the Washington RGB-D Object Dataset.},
author = {Aakerberg, Andreas and Nasrollahi, Kamal and Rasmussen, Christoffer B. and Moeslund, Thomas B.},
doi = {10.5220/0006511501210128},
isbn = {978-989-758-274-5},
journal = {Proceedings of the 9th International Joint Conference on Computational Intelligence},
keywords = {artificial vision,computer vision,convolutional neural networks,deep learning,has found enormous applications,in computer vision which,learning,object recognition is one,of the important tasks,rgb-d,surface normals,transfer},
pages = {121--128},
title = {{Depth Value Pre-Processing for Accurate Transfer Learning based RGB-D Object Recognition}},
url = {http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0006511501210128},
year = {2017}
}
@article{Saha2017,
annote = {Not a very good article. but it has some nice references.

Keypoints

It is possible to aquire iris images in multiple way including Near Infrared (NIRD). a simple lense and monochome CCD camera, Adaboost cascade iris detector.

Iris localization is done by Daugman using a 2D Gabor Filter and Fisher Linear Discriminate method.

To help localize the iris despite of eyelashes a 1D rankfilter and histogram filter can be used.},
author = {Saha, Rishmita and Kundu, Mahasweta and Dutta, Madhuparna and Majumder, Rahul and Mukherjee, Debosmita and Pramanik, Sayak and Thakur, Uttam Narendra and Mukherjee, Chiradeep},
file = {:C$\backslash$:/Users/Shaggy/Desktop/Iris Articles/A brief study on evolution of iris recognition system.pdf:pdf},
isbn = {9781538633717},
journal = {Information Technology, Electronics and Mobile Communication Conference (IEMCON), 2017 8th IEEE Annual},
pages = {685--688},
title = {{A Brief Study on Evolution of Iris Recognition System}},
url = {http://ieeexplore.ieee.org.ezproxy.psu.edu.sa/stamp/stamp.jsp?arnumber=8117234},
year = {2017}
}
@article{Karpathy2016a,
abstract = {These notes accompany the Stanford CS class CS231n: Convolutional Neural Networks for Visual Recognition.},
author = {Karpathy, Andrej},
journal = {Stanford University},
pages = {1--21},
title = {{Convolutional Neural Networks for Visual Recognition}},
url = {http://cs231n.github.io/classification/},
year = {2016}
}
@article{Wrobel2017a,
abstract = {{\textcopyright} 2017 IEEE. In this paper, a new approach for personal identity verification using finger knuckle images and least-square contour alignment method has been proposed. A special test rig with a digital camera was prepared for acquisition the knuckle images. Next, the obtained images of finger knuckle were subjected to image processing method in order to extract the knuckle furrows from them. The verification of person was performed by comparing the furrows on the verified and the reference knuckle images. To determine the similarity between the furrows we used the least-square contour alignment method. The usability of the proposed approach was tested experimentally. Practical experiments, conducted with our database, confirmed that results obtained are promising.},
author = {Wrobel, K. and Porwik, P. and Doroz, R. and Safaverdi, H.},
doi = {10.1109/ICBAKE.2017.8090616},
isbn = {9781538634004},
journal = {Proceedings of 2017 International Conference on Biometrics and Kansei Engineering, ICBAKE 2017},
keywords = {biometrics,finger knuckle images,least-square contour alignment,verification},
pages = {119--122},
title = {{Person verification based on finger knuckle images and least-squares contour alignment}},
year = {2017}
}
@article{Wan2017a,
abstract = {State-of-the-art methods for 3D hand pose estimation from depth images require large amounts of annotated training data. We propose to model the statistical relationships of 3D hand poses and corresponding depth images using two deep generative models with a shared latent space. By design, our architecture allows for learning from unlabeled image data in a semi-supervised manner. Assuming a one-to-one mapping between a pose and a depth map, any given point in the shared latent space can be projected into both a hand pose and a corresponding depth map. Regressing the hand pose can then be done by learning a discriminator to estimate the posterior of the latent pose given some depth map. To improve generalization and to better exploit unlabeled depth maps, we jointly train a generator and a discriminator. At each iteration, the generator is updated with the back-propagated gradient from the discriminator to synthesize realistic depth maps of the articulated hand, while the discriminator benefits from an augmented training set of synthesized and unlabeled samples. The proposed discriminator network architecture is highly efficient and runs at 90 FPS on the CPU with accuracies comparable or better than state-of-art on 3 publicly available benchmarks.},
archivePrefix = {arXiv},
arxivId = {1702.03431},
author = {Wan, Chengde and Probst, Thomas and {Van Gool}, Luc and Yao, Angela},
doi = {10.1109/CVPR.2017.132},
eprint = {1702.03431},
journal = {Cvpr2017},
pages = {10},
title = {{Crossing Nets: Dual Generative Models with a Shared Latent Space for Hand Pose Estimation}},
url = {http://arxiv.org/abs/1702.03431},
year = {2017}
}
@article{LeCun2015,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6184v5},
author = {Lecun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
doi = {10.1038/nature14539},
eprint = {arXiv:1312.6184v5},
file = {:C$\backslash$:/Users/Shaggy/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/LeCun, Bengio, Hinton - 2015 - Deep learning.pdf:pdf},
isbn = {9780521835688},
issn = {14764687},
journal = {Nature},
keywords = {Computer science,Mathematics and computing},
month = {may},
number = {7553},
pages = {436--444},
pmid = {10463930},
publisher = {Nature Publishing Group},
title = {{Deep learning}},
url = {http://www.nature.com/articles/nature14539},
volume = {521},
year = {2015}
}
@article{Zhao2017b,
abstract = {Person re-identification (ReID) is an important task in video surveillance and has various applications. It is non-trivial due to complex background clutters, varying illu-mination conditions, and uncontrollable camera settings. Moreover, the person body misalignment caused by detec-tors or pose variations is sometimes too severe for feature matching across images. In this study, we propose a novel Convolutional Neural Network (CNN), called Spindle Net, based on human body region guided multi-stage feature de-composition and tree-structured competitive feature fusion. It is the first time human body structure information is con-sidered in a CNN framework to facilitate feature learning. The proposed Spindle Net brings unique advantages: 1) it separately captures semantic features from different body regions thus the macro-and micro-body features can be well aligned across images, 2) the learned region features from different semantic regions are merged with a competitive scheme and discriminative features can be well preserved. State of the art performance can be achieved on multiple datasets by large margins. We further demonstrate the ro-bustness and effectiveness of the proposed Spindle Net on our proposed dataset SenseReID without fine-tuning.},
author = {Zhao, Haiyu and Tian, Maoqing and Sun, Shuyang and Shao, Jing and Yan, Junjie and Yi, Shuai and Wang, Xiaogang and Tang, Xiaoou},
doi = {10.1109/CVPR.2017.103},
isbn = {978-1-5386-0457-1},
journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
pages = {907--915},
title = {{Spindle Net: Person Re-identification with Human Body Region Guided Feature Decomposition and Fusion}},
url = {http://ieeexplore.ieee.org/document/8099586/},
volume = {1},
year = {2017}
}
@article{Gulmire2012,
author = {Gulmire, Kshamaraj and Ganorkar, Sanjay},
file = {::},
issn = {2278-0181},
number = {5},
pages = {1--5},
title = {{Iris Recognition Using Gabor Wavelet}},
volume = {1},
year = {2012}
}
@article{Al-Waisy2017a,
abstract = {Multimodal biometrie systems seek to alleviate some of the limitations of unimodal biometrie systems by combining multiple pieces of evidence of the same person in the deeision-making process. In this paper, a novel multimodal biometric identification system is proposed based on fusing the results obtained from both the face and the left and right irises using deep learning approaches. Firstly, the facial features are extracted using a Deep Belief Network (DBN) architecture consisting of 3-layers. The first two RBMs are used as features detectors, while the last one is used as a discriminative model associated with softmax units for the multi-class classification purpose. Secondly, an efficient deep learning system is employed for iris recognition, whose architecture is based on a combination of Convolutional Neural Network (CNN) and Softmax classifier to extract discriminative features from an iris image. Extensive experiments on large-scale challenging databases, including FERET, CASIA V1.0 and MMU1, and SDUMLA-HMT have demonstrated the superiority of the proposed approaches by achieving new state-of-the-art Rank-1 identification rates on all the employed databases.},
author = {Al-Waisy, A S and Qahwaji, R and Ipson, S and Al-Fahdawi, S},
doi = {10.1109/EST.2017.8090417},
isbn = {9781538640173},
journal = {2017 Seventh International Conference on Emerging Security Technologies (EST)},
keywords = {Databases;Face;Feature extraction;Iris recognition},
pages = {163--168},
title = {{A multimodal biometrie system for personal identification based on deep learning approaches}},
year = {2017}
}
@inproceedings{Kaur2016,
abstract = {Abstract: Face recognition is a type of biometric software application by using which, we can analyzing, identifying or verifying digital image of the person by using the feature of the face of the person that are unique characteristics of each person. These characteristics may be physical or behavioral. The physiological characteristics as like finger print, iris scan, or face etc and behavior characteristics as like hand-writing, voice, key stroke etc. Face recognition is very useful in many areas such as military, airports, universities, ATM, and banks etc, used for the security purposes. There are many techniques or algorithms that are used features extraction in face recognition. This paper make a review of some of those methods which are used for the face recognition that are Principal Component Analysis (PCA), Back Propagation Neural Networks (BPNN), Genetic Algorithm, and LDA, SVM, Independent Component Analysis(ICA). Each method has different -2 functions that are used for the face recognition. Dimensionality is reduced by using the Eigen face approach or PCA, LDA to extract the features from images. Genetic Algorithm is based on feature selection and Back propagation Neural Network (BPNN) is used for the classification of face images.},
author = {Kaur, Gurpreet and Kanwal, Navdeep},
booktitle = {International Conference on Computing for Sustainable Global Development (INDIACom)},
isbn = {9789380544212},
pages = {2705--2710},
publisher = {IEEE},
title = {{A Comparative Review of Various Approaches for Feature Extraction in Face Recognition}},
url = {http://ieeexplore.ieee.org/document/7724754/},
year = {2016}
}
@article{Yang2017a,
abstract = {We present an end-to-end, multimodal, fully convolutional$\backslash$r$\backslash$nnetwork for extracting semantic structures from document$\backslash$r$\backslash$nimages. We consider document semantic structure$\backslash$r$\backslash$nextraction as a pixel-wise segmentation task, and propose a$\backslash$r$\backslash$nunified model that classifies pixels based not only on their$\backslash$r$\backslash$nvisual appearance, as in the traditional page segmentation$\backslash$r$\backslash$ntask, but also on the content of underlying text. Moreover,$\backslash$r$\backslash$nwe propose an efficient synthetic document generation process$\backslash$r$\backslash$nthat we use to generate pretraining data for our network.$\backslash$r$\backslash$nOnce the network is trained on a large set of synthetic$\backslash$r$\backslash$ndocuments, we fine-tune the network on unlabeled real documents$\backslash$r$\backslash$nusing a semi-supervised approach. We systematically$\backslash$r$\backslash$nstudy the optimum network architecture and show that$\backslash$r$\backslash$nboth our multimodal approach and the synthetic data pretraining$\backslash$r$\backslash$nsignificantly boost the performance.},
archivePrefix = {arXiv},
arxivId = {1706.02337},
author = {Yang, Xiao and Yumer, Ersin and Asente, Paul and Kraley, Mike and Kifer, Daniel and Giles, C. Lee},
doi = {10.1109/CVPR.2017.462},
eprint = {1706.02337},
isbn = {978-1-5386-0457-1},
journal = {Cvpr2017},
title = {{Learning to Extract Semantic Structure from Documents Using Multimodal Fully Convolutional Neural Networks}},
url = {https://arxiv.org/pdf/1706.02337.pdf},
year = {2017}
}
@article{Kaur2016a,
abstract = {Face recognition is a type of biometric software application by using which, we can analyzing, identifying or verifying digital image of the person by using the feature of the face of the person that are unique characteristics of each person. These characteristics may be physical or behavioral. The physiological characteristics as like finger print, iris scan, or face etc and behavior characteristics as like hand-writing, voice, key stroke etc. Face recognition is very useful in many areas such as military, airports, universities, ATM, and banks etc, used for the security purposes. There are many techniques or algorithms that are used features extraction in face recognition. This paper make a review of some of those methods which are used for the face recognition that are Principal Component Analysis (PCA), Back Propagation Neural Networks (BPNN), Genetic Algorithm, and LDA, SVM, Independent Component Analysis(ICA). Each method has different -2 functions that are used for the face recognition. Dimensionality is reduced by using the Eigen face approach or PCA, LDA to extract the features from images. Genetic Algorithm is based on feature selection and Back propagation Neural Network (BPNN) is used for the classification of face images.},
author = {Kaur, Gurpreet and Kanwal, Navdeep},
file = {:C$\backslash$:/Users/Shaggy/Desktop/Iris Articles/A comparative review of various approaches for feature extraction in face recognition.pdf:pdf},
isbn = {9789380544212},
journal = {International Conference on Computing for Sustainable Global Development (INDIACom)},
keywords = {BPNN,Face recognition,Features extraction,LDA,PCA},
pages = {2705--2710},
title = {{A comparative review of various approaches for feature extraction in face recognition}},
url = {http://ieeexplore.ieee.org/document/7724754/},
year = {2016}
}
@article{Kauba2016,
abstract = {—Authentication based on vein patterns is a very promising biometric technique. The most important step is the accurate extraction of the vein pattern from sometimes low quality input images. A single feature extraction technique may fail to correctly extract the vein pattern, entailing bad recognition performance. One of the solutions that can be used to improve recognition results is biometric fusion. A possible fusion strategy is feature level fusion, that is the fusion of several feature extractors' outputs. In our work, we exploited the feature level fusion to improve the quality of the extracted vein patterns and thus the feature extraction accuracy. An experimental study involving different feature extraction techniques (maximum curvature, repeated line tracking, wide line detector, ...) and different fusion techniques (majority voting, weighted average, STAPLE, ...) is conducted on the UTFVP finger-vein data set. The results show that feature level fusion is able to improve the recognition accuracy in terms of the EER over the single feature extraction techniques.},
author = {Kauba, Christof and Uhl, Andreas and Piciucco, Emanuela and Maiorana, Emanuele and Campisi, Patrizio},
doi = {10.1109/BIOSIG.2016.7736908},
isbn = {9783885796541},
issn = {16175468},
journal = {Lecture Notes in Informatics (LNI), Proceedings - Series of the Gesellschaft fur Informatik (GI)},
title = {{Advanced variants of feature level fusion for finger vein recognition}},
volume = {P-260},
year = {2016}
}
@article{Vivek2012a,
author = {Vivek, S. Arun and Aravinth, J. and Valarmathy, S.},
doi = {10.1109/ICPRIME.2012.6208377},
isbn = {978-1-4673-1039-0},
journal = {International Conference on Pattern Recognition, Informatics and Medical Engineering (PRIME-2012)},
keywords = {density based score level,error,feature extraction,fusion,gmm,likelihood ratio test,multimodal biometrics,rates,template,unimodal biometrics},
pages = {387--392},
title = {{Feature extraction for multimodal biometric and study of fusion using Gaussian mixture model}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6208377},
year = {2012}
}
@article{Galdi2017,
abstract = {FIRE is a Fast Iris REcognition algorithm especially designed for iris recognition on mobile phones under visible-light. It is based on the combination of three classifiers exploiting the iris colour and texture information. Its limited computational time makes FIRE particularly suitable for fast user verification on mobile devices. The high parallelism of the code allows its use also on large databases. FIRE, in its first version, was submitted to the Mobile Iris CHallenge Evaluation part II held in 2016. In this paper, FIRE is further improved: a number of different techniques has been analyzed and the best performing ones have been selected for fusion at score level. Performance are assessed in terms of Recognition Rate (RR), Area Under Receiver Operating Characteristic Curve (AUC), and Equal Error Rate (EER).},
author = {Galdi, Chiara and Dugelay, Jean Luc},
doi = {10.1016/j.patrec.2017.01.023},
file = {:C$\backslash$:/Users/Shaggy/Desktop/Iris Articles/FIRE Fast Iris REcognition on mobile phones by combining colour and texture features.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Fast iris recognition,MICHE DB,MICHE II,Multi-classifier,Noisy iris recognition,Visible light},
pages = {44--51},
publisher = {Elsevier B.V.},
title = {{FIRE: Fast Iris REcognition on mobile phones by combining colour and texture features}},
url = {http://dx.doi.org/10.1016/j.patrec.2017.01.023},
volume = {91},
year = {2017}
}
@article{B2017,
author = {B, Qi Wang and Su, Xia and Cai, Zhenlin and Zhang, Xiangde},
doi = {10.1007/978-3-319-69923-3},
file = {:C$\backslash$:/Users/Shaggy/Desktop/Iris Articles/Mobile Iris Recognition via Fusing Different Kinds of Features.pdf:pdf},
isbn = {978-3-319-69922-6},
keywords = {joint bayesian,mobile iris recognition,ordinal measures},
number = {3},
pages = {401--410},
title = {{Biometric Recognition}},
url = {http://link.springer.com/10.1007/978-3-319-69923-3},
volume = {10568},
year = {2017}
}
@inproceedings{Trokielewicz2016,
author = {Trokielewicz, Mateusz},
booktitle = {2016 IEEE International Conference on Identity, Security and Behavior Analysis (ISBA)},
doi = {10.1109/ISBA.2016.7477233},
isbn = {978-1-4673-9727-8},
month = {feb},
pages = {1--6},
publisher = {IEEE},
title = {{Iris recognition with a database of iris images obtained in visible light using smartphone camera}},
url = {http://ieeexplore.ieee.org/document/7477233/},
year = {2016}
}
@article{Chen2005a,
abstract = {The recognition accuracy of a single biometric authentication system is often much reduced due to the environment, user mode and physiological defects. In this paper, we combine face and iris features for developing a multimode biometric approach, which is able to diminish the drawback of single biometric approach as well as to improve the performance of authentication system. We combine a face database ORL and iris database CASIA to construct a multimodal biometric experimental database with which we validate the proposed approach and evaluate the multimodal biometrics performance. The experimental results reveal the multimodal biometrics verification is much more reliable and precise than single biometric approach.},
annote = {Face and iris for authentication 
uses synthetic multimodal biometric dataset

Shows how to evaluate system. 

proves multimodal performs better.},
author = {Chen, Ching-Han and {Te Chu}, Chia},
doi = {10.1007/11608288_76},
isbn = {978-3-540-31621-3},
issn = {03029743},
keywords = {face,iris,multimodal biometrics,wavelet probabilistic neural},
pages = {571--580},
title = {{Fusion of Face and Iris Features for Multimodal Biometrics}},
url = {http://link.springer.com/10.1007/11608288{\_}76},
year = {2005}
}
@article{Soviany2016,
abstract = {– This paper presents a design approach of a reliable authentication system for mobile applications (such as those within m-Health or m-Banking areas). This means that the biometric data processing should optimize the security performance vs. the computational complexity. The security is given by the combination of fingerprint, iris and voice features that define the multimodal pattern of an individual. The complexity reduction is supported by a reduced feature space, especially for the fingerprint and iris recognition components of the overall system.},
author = {Soviany, Sorin and Săndulescu, Virginia and Puşcoci, Sorin},
file = {:C$\backslash$:/Users/Shaggy/Desktop/Iris Articles/A multimodal biometric identification method for mobile applications security.pdf:pdf},
isbn = {9781509020478},
journal = {Computers and Artificial Intelligence},
keywords = {-multimodal,biometrics,data fusion,identification},
title = {{A Multimodal Biometric Identification Method for Mobile Applications Security}},
volume = {30},
year = {2016}
}
@article{Dessimoz2007,
abstract = {The MBioID initiative has been set up to address the following germane question: What and how biometric technologies could be deployed in identity documents in the foreseeable future? This research effort proposes to look at current and future practices and systems of establishing and using biometric identity documents (IDs) and evaluate their effectiveness in large-scale developments. The first objective of the MBioID project is to present a review document establishing the current state-of-the-art related to the use of multimodal biometrics in an IDs application. This research report gives the main definitions, properties and the framework of use related to biometrics, an overview of the main standards developed in the biometric industry and standardisation organisations to ensure interoperability, as well as some of the legal framework and the issues associated to biometrics such as privacy and personal data protection. The state-of-the-art in terms of technological development is also summarised for a range of single biometric modalities (2D and 3D face, fingerprint, iris, on-line signature and speech), chosen according to ICAO recommendations and availabilities, and for various multimodal approaches. This paper gives a summary of the main elements of that report. The second objective of the MBioID project is to propose relevant acquisition and evaluation protocols for a large-scale deployment of biometric IDs. Combined with the protocols, a multimodal database will be acquired in a realistic way, in order to be as close as possible to a real biometric IDs deployment. In this paper, the issues and solutions related to the acquisition setup are briefly presented. {\textcopyright} 2006 Elsevier Ireland Ltd. All rights reserved.},
author = {Dessimoz, Damien and Richiardi, Jonas and Champod, Christophe and Drygajlo, Andrzej},
doi = {10.1016/j.forsciint.2006.06.037},
issn = {03790738},
journal = {Forensic Science International},
keywords = {Acquisition protocol,Biometrics,Electronic passport,Evaluation protocol,Identity documents,Multimodality},
number = {2-3},
pages = {154--159},
pmid = {16890391},
title = {{Multimodal biometrics for identity documents ({\{}A figure is presented{\}})}},
volume = {167},
year = {2007}
}
@article{Hu2015,
abstract = {Deep learning, in particular Convolutional Neural Network (CNN), has achieved promising results in face recognition recently. However, it remains an open question: why CNNs work well and how to design a 'good' architecture. The existing works tend to focus on reporting CNN architectures that work well for face recognition rather than investigate the reason. In this work, we conduct an extensive evaluation of CNN-based face recognition systems (CNN-FRS) on a common ground to make our work easily reproducible. Specifically, we use public database LFW (Labeled Faces in the Wild) to train CNNs, unlike most existing CNNs trained on private databases. We propose three CNN architectures which are the first reported architectures trained using LFW data. This paper quantitatively compares the architectures of CNNs and evaluate the effect of different implementation choices. We identify several useful properties of CNN-FRS. For instance, the dimensionality of the learned features can be significantly reduced without adverse effect on face recognition accuracy. In addition, traditional metric learning method exploiting CNN-learned features is evaluated. Experiments show two crucial factors to good CNN-FRS performance are the fusion of multiple CNNs and metric learning. To make our work reproducible, source code and models will be made publicly available.},
archivePrefix = {arXiv},
arxivId = {1504.02351},
author = {Hu, Guosheng and Yang, Yongxin and Yi, Dong and Kittler, Josef and Christmas, William and Li, Stan Z. and Hospedales, Timothy},
doi = {10.1109/ICCVW.2015.58},
eprint = {1504.02351},
file = {::},
isbn = {9780769557205},
issn = {15505499},
journal = {2015 IEEE International Conference on Computer Vision Workshop (ICCVW)},
month = {dec},
pages = {384--392},
publisher = {IEEE},
title = {{When Face Recognition Meets with Deep Learning: an Evaluation of Convolutional Neural Networks for Face Recognition}},
url = {http://ieeexplore.ieee.org/document/7406407/ http://arxiv.org/abs/1504.02351},
year = {2015}
}
@article{Uka2017,
annote = {Keynotes:

CASIA (Most used database) and IIT Delhi Iris Database used.

Hough Transofrm algorithm used to detect boundaries},
author = {Uka, Arban and Ro{\c{c}}i, Albana and Ko{\c{c}}, Oktay},
file = {:C$\backslash$:/Users/Shaggy/Desktop/Iris Articles/Improved segmentation algorithm and further optimization for iris recognition.pdf:pdf},
isbn = {9781509038435},
journal = {IEEE EUROCON 2017 -17th International Conference on Smart Technologies},
keywords = {encoding,equal error rate,segmentation},
number = {July},
pages = {6--8},
title = {{Improved Segmentation Algorithm and Further Optimization for Iris Recognition}},
year = {2017}
}
